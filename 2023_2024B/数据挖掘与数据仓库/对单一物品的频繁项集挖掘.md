# 对单一物品所在的频繁项集挖掘的方法

课程：数据挖掘与数据仓库

班级：2021级软件工程（普通）2班

学号：222021321262043

姓名：杨锦烨

[toc]

## 引言

目前我们所学频繁项集挖掘算法均为针对某一数据集，在给定最小支持度后求出所对应的频繁项集。其对应的实际场景有著名的“啤酒与尿不湿”的案例，在挖掘出指定的频繁项集后可以对实际的生产生活，如超市摆货起到指导作用。但是对每一组大于最小支持度的频繁项集进行挖掘会消耗大量的时间，实际上在某些特殊情况下，针对某一特定商品的挖掘也能起到作用。如对于2024年欧洲杯，可以预见的是体育商店的足球销量会激增，那么对于足球这一个商品来说，挖掘出只与足球有关的频繁项集，就能在消耗较少时间的情况下，得到对应的对体育商店围绕足球这单一热门产品的商品摆货进行提出指导意见。

本次实验主要针对所学的Apriori算法来进行改进，让其只对特定的商品进行挖掘而不是挖掘全部的频繁项集，从而减少时间的消耗。主要思想是在其第K轮进行候选集选取时，长度为K-1的前缀不含有目标项的候选集即被剪枝。而原算法是对长度为K-1的前缀相同的候选项集检查其对称差是否在上一个候选集中，若不在则剪枝。

## 算法

### Apriori算法重述

对于Apriori算法的思路大概如下：

首先有两个定律：

Apriori定律1 ：如果某商品组合小于最小支持度，则就将它舍去，它的超集必然不是频繁项集。
Apriori定律2 ：如果一个集合是频繁项集，即这个商品组合支持度大于最小支持度，则它的所有子集都是频繁项集

实现算法的过程也有两步：

1. 找出所有频繁项集。
   就是该商品组合的支持度 大于 最小支持度

2. 由频繁项集确定下一组候选集。

可以由下图概括：

![P8](D:\Typora\notes\2023_2024B\数据挖掘与数据仓库\pics\P8.png) 

具体过程如下：

1. 第一次扫描
   首先，求第一次扫描数据库后的**候选集**。第一次扫描后，可以求出单个商品的支持度（图中支持度用出现次数表示），这个表称为**第一次候选集**

2. 在第一次候选集基础上，求出第一次频繁项集

   频繁项就是该商品的支持度 大于 最小支持度，支持度选择是随意的，在这里取最小支持度为 min_support=2。那么**第一次频繁项集**就是第一次候选集中，支持度大于或等于2的所有商品集合。

3. 求出**第二次的候选集**
   即在第一次频繁项集的基础上，找出第二次候选集，对商品进行**组合**，形成一个2元组，4种商品，不同组合有$C_{4}^{2}$种，即 4\*3=12 种，形成的表称为第二次候选集表。

4. 求**第二次频繁项集**
   求出这两种商品同时出现在总记录中的次数（即求支持度），然后去掉支持度小于2的商品组合，形成的表即为第二次频繁项集。

5. 即在第二次频繁项集的基础上，找出第三次候选集

   设K为第K次扫描，要求第K个候选集，找出上一次扫描的频繁项集，然后观察里面的记录，对于里面的每个记录，前（K-2）个前缀相同的，归为一类，在同一类别中进行合并。比如第三次扫描，要求出它的候选集，先找出上次扫描形成的第二次频繁项集表，里面有4条记录，分别为AC，BC，BE，CE，这些记录中，前（K-2）个前缀，就是前（3-2）个前缀，也就是第一个前缀相同的归为一类，接着在属于同一类的记录中，进行合并，**比如BC，BE，它们的第一个前缀都是B，那么在这一类中，把它们合并起来就形成了BCE**。还剩下AC、CE，这两项的第一个前缀不相同，也没有其他元素和它们相同，那么就忽视这两项。如果合并这两项，把AC、CE合并为ACE，它的子集是{AC、CE、AE}，其中AE不是频繁项集。那么ACE也必然不是频繁项集。（Apriori定律1 ：如果某商品组合小于最小支持度，则就将它舍去，它的超集必然不是频繁项集。）

6. 重复以上步骤直到候选集中只剩1个或0个



### Single-Apriori

本次实验的创新点在于对经典的Apriori进行改进，让其只对特定的商品进行挖掘而不是挖掘全部的频繁项集。即在上述过程的第2步开始，产生第二次的候选集时，不将不含目标项的项集加入候选集。即对于下图，假设目标项是2，那么在筛选目标项时，对于其K-1前缀中不含2的项集，就直接选择不加入候选集。因为原项集集合在按字典序排序，所以不会出现不含2的项集在被筛去之后导致漏解。似乎对于不含2的项集，如(3, 4)，直接将其删去，如果还有项集(3, 2)就可能会漏掉(3, 2, 4)的候选项集，但是实际上如果(3, 2, 4)为频繁项集，那么前面的(2, 3)和(2, 4)按字典序就会先组成(2, 3, 4)的频繁项集，所以按字典序排序后删掉其K-1前缀中不含2的项集并不会影响生成含2的频繁项集。

那么改进后的算法可以减去不含目标项的计算消耗，在算法上对后续的空间大幅剪枝。但是由于减去了候选集中其它项，所以apriori算法中对两个候选项集的对称差在候选集中验证这一步就必须省去。

**实际例子如下：**

假设某个数据集在第二轮扫描后的项集后情况如下表：

| (1, 2) |  2   |
| :----: | :--: |
| (1, 3) |  3   |
| (1, 5) |  4   |
| (2, 3) |  0   |
| (2, 4) |  2   |
| (3, 4) |  3   |
| (3, 5) |  4   |
| (4, 5) |  2   |

如果对apriori算法，筛除了支持度低于minsup = 2的项集后按apriori的算法生成候选集如下，其中如对于(1, 2)和(1, 5)生成的候选集(1, 2, 5)因为两者的对称差集合(2, 5)项集支持度为0，不符合apriori定律所以没有生成候选集：

| (1, 2, 3) |
| :-------: |
| (1, 3, 5) |
| (2, 3, 4) |
| (3, 4, 5) |

如果用改进的只对目标项2进行筛选的apriori算法，生成的候选集如下：
| (1, 2, 3) |
| :-------: |
| (2, 3, 4) |

可以看到比之原算法候选集有所减少。

### 伪代码

注意：因markdown无法直接插入矢量图形式的伪代码，所以采用在latex中编写好后截图的形式

![P3](D:\Typora\notes\2023_2024B\数据挖掘与数据仓库\pics\P3.png)

![P4](D:\Typora\notes\2023_2024B\数据挖掘与数据仓库\pics\P4.png)

![P5](D:\Typora\notes\2023_2024B\数据挖掘与数据仓库\pics\P5.png)

![P6](D:\Typora\notes\2023_2024B\数据挖掘与数据仓库\pics\P6.png)

![P7](D:\Typora\notes\2023_2024B\数据挖掘与数据仓库\pics\P7.png)

## 代码

### apriori算法代码

```python
def item(dataset):  # 求第一次扫描数据库后的 候选集，（它没法加入循环）
    c1 = []  # 存放候选集元素
    # 求这个数据库中出现了几个元素，然后返回
    for x in dataset:  # 逐行遍历每条数据
        for y in x:  # 取出每条数据中每个商品，放入c1
            if [y] not in c1:
                c1.append([y])
    c1.sort()
    # print(c1)
    return c1


def get_frequent_item(dataset, c, min_support):
    # dataset:原始数据集， c:上一次产生的候选集
    cut_branch = {}  # 用来存放所有项集的支持度的字典
    for x in c:  # 遍历候选集中的每一条前缀数据
        for y in dataset:  # 遍历原始数据的每一条
            if set(x).issubset(set(y)):  # 如果 x 不在 y中，就把对应元素后面加 1
                cut_branch[tuple(x)] = cut_branch.get(tuple(x), 0) + 1  # 给候选集增加支持度

    Fk = []  # 支持度大于最小支持度的项集，即频繁项集
    sup_dataK = {}  # 用来存放所有 频繁 项集的支持度的字典

    for i in cut_branch:  # i为key
        if cut_branch[i] >= min_support:  # Apriori定律1  小于支持度，则就将它舍去，它的超集必然不是频繁项集
            Fk.append(list(i))
            sup_dataK[i] = cut_branch[i]
    # print(Fk)
    return Fk, sup_dataK


def get_candidate(Fk, K):  # 求第k次候选集
    ck = []  # 存放产生候选集
    # 两重循环对候选集Fk中每个都尝试两两组合，看是否可以凑出新的候选集（长度+1）
    for i in range(len(Fk)):
        for j in range(i + 1, len(Fk)):
            L1 = list(Fk[i])[:K - 2]  # 按字典序查看前K - 1个
            L2 = list(Fk[j])[:K - 2]
            L1.sort()  # 排序是为了按字典序
            L2.sort()  # 先排序，在进行组合

            if L1 == L2:
                if K > 2:  # 第二次求候选集，不需要进行减枝，因为第一次候选集都是单元素，且已经减枝了，组合为双元素肯定不会出现不满足支持度的元素
                    new = list(set(Fk[i]) ^ set(Fk[j]))  # 集合运算 对称差集 ^ （含义，集合的元素在t或s中，但不会同时出现在二者中）
                    # new表示，这两个记录中，不同的元素集合
                    # 为什么要用new？ 比如 1，2     1，3  两个合并成 1，2，3   我们知道1，2 和 1，3 一定是频繁项集，但 2，3呢，我们要判断2，3是否为频繁项集
                    # Apriori定律1 如果一个集合不是频繁项集，则它的所有超集都不是频繁项集
                else:  # 第二次求候选集的时候
                    new = set()
                for x in Fk:
                    if set(new).issubset(set(x)) and list(set(Fk[i]) | set(Fk[j])) not in ck:
                        # 减枝 new是 x 的子集，并且 还没有加入 ck 中
                        ck.append(list(set(Fk[i]) | set(Fk[j])))
    return ck


def Apriori(dataset, min_support=2):
    c1 = item(dataset)  # 返回一个二维列表，里面的每一个一维列表，都是第一次候选集的元素, 此时还没有排除支持度小于阈值的
    f1, sup_1 = get_frequent_item(dataset, c1, min_support)  # 求第一次候选集
    F = [f1]  # 将第一次候选集产生的频繁项集放入 F ,以后每次扫描产生的所有频繁项集都放入里面, f1也为一个list，所以F是二维数组
    sup_data = sup_1  # 一个字典，里面存放所有产生的候选集，及其支持度

    K = 2  # 从第二个开始循环求解，先求候选集，在求频繁项集

    while len(F[K - 2]) > 1:  # k-2是因为F是从0开始数的前一个的频繁项集个数在2个或2个以上，才继续循环，否则退出
        ck = get_candidate(F[K - 2], K)  # 求第k次候选集
        fk, sup_k = get_frequent_item(dataset, ck, min_support)  # 求第k次频繁项集

        F.append(fk)  # 把新产生的候选集假如F
        sup_data.update(sup_k)  # 字典更新，加入新得出的数据
        K += 1
    return F, sup_data  # 返回所有频繁项集， 以及存放频繁项集支持度的字典
```



### 针对目标项求对应频繁项集的apriori算法代码

生成初始候选集：

```python
def item(dataset):  # 求第一次扫描数据库后的候选集，（它没法加入循环）
    c1 = []  # 存放候选集元素
    # 求这个数据库中出现了几个元素，然后返回
    for x in dataset:  # 逐行遍历每条数据
        for y in x:  # 取出每条数据中每个商品，放入c1
            if [y] not in c1:
                c1.append([y])
    c1.sort()
    # print(c1)
    return c1
```

根据上一次的候选集生成频繁项集：

```python
def get_frequent_item(dataset, c, min_support):
    # dataset:原始数据集， c:上一次产生的候选集
    cut_branch = {}  # 用来存放所有项集的支持度的字典
    for x in c:  # 遍历候选集中的每一条前缀数据
        for y in dataset:  # 遍历原始数据的每一条
            if set(x).issubset(set(y)):  # 如果 x 不在 y中，就把对应元素后面加 1
                cut_branch[tuple(x)] = cut_branch.get(tuple(x), 0) + 1  # 给候选集增加支持度

    Fk = []  # 支持度大于最小支持度的项集，即频繁项集
    sup_dataK = {}  # 用来存放所有 频繁 项集的支持度的字典

    for i in cut_branch:  # i为key
        if cut_branch[i] >= min_support:  # Apriori定律1  小于支持度，则就将它舍去，它的超集必然不是频繁项集
            Fk.append(list(i))
            sup_dataK[i] = cut_branch[i]
    # print(Fk)
    return Fk, sup_dataK
```

生成第K轮扫描时的候选集：

```python
def my_get_candidate(Fk, target_item, K):  # 求第k次候选集
    ck = []  # 存放产生候选集
    # 两重循环对候选集Fk中每个都尝试两两组合，看是否可以凑出新的候选集（长度+1）
    for i in range(len(Fk)):
        for j in range(i + 1, len(Fk)):
            L1 = list(Fk[i])[:K - 2]  # 按字典序查看前K - 1个
            L2 = list(Fk[j])[:K - 2]
            L1.sort()  # 排序是为了按字典序
            L2.sort()  # 先排序，在进行组合

            if L1 == L2:
                # 检查是否是target_item是否在组合取并集之后的集合内，如果在，那么就将该集合加入候选集
                if target_item in list(set(Fk[i]) | set(Fk[j])) and list(set(Fk[i]) | set(Fk[j])) not in ck:
                    ck.append(list(set(Fk[i]) | set(Fk[j])))


    return ck
```

针对目标项求对应频繁项集的apriori算法：

```python
def my_Apriori(dataset, target_item, min_support=2):
    c1 = item(dataset)  # 返回一个二维列表，里面的每一个一维列表，都是第一次候选集的元素, 此时还没有排除支持度小于阈值的
    f1, sup_1 = get_frequent_item(dataset, c1, min_support)  # 求第一次候选集
    F = [f1]  # 将第一次候选集产生的频繁项集放入 F ,以后每次扫描产生的所有频繁项集都放入里面, f1也为一个list，所以F是二维数组
    ans_F = [[target_item]]
    sup_data = sup_1  # 一个字典，里面存放所有产生的候选集，及其支持度
    ans_sup_data = {}
    ans_sup_data[tuple([target_item])] = sup_1[tuple([target_item])]

    K = 2  # 从第二个开始循环求解，先求候选集，在求频繁项集

    while len(F[K - 2]) > 1:  # k-2是因为F是从0开始数的前一个的频繁项集个数在2个或2个以上，才继续循环，否则退出
        ck = my_get_candidate(F[K - 2], target_item, K)  # 求第k次候选集
        fk, sup_k = get_frequent_item(dataset, ck, min_support)  # 求第k次频繁项集

        F.append(fk)  # 把新产生的候选集假如F
        ans_F.append(fk)
        sup_data.update(sup_k)  # 字典更新，加入新得出的数据
        ans_sup_data.update(sup_k)
        K += 1
    return ans_F, ans_sup_data  # 返回所有频繁项集， 以及存放频繁项集支持度的字典
```

定义生成dataset的函数，生成40条数据，每条数据包含商品数量为1-10，商品的编号为1-50：

```python
def generate_dataset(num_items=40):
    dataset = []
    for _ in range(num_items):
        # 随机生成子列表的长度，范围在1到20之间
        sub_list_length = random.randint(1, 10)
        # 根据子列表长度生成随机数列表
        sub_list = [random.randint(1, 50) for _ in range(sub_list_length)]
        # sub_list.sort()
        dataset.append(sub_list)
    return dataset
```

```python
if __name__ == '__main__':
    # 调用函数生成dataset
    dataset = generate_dataset()
    # 打印生成的dataset
    print(dataset)
    target_item = 2 # 假设目标项集为2

    t = time.time()
    my_F, my_sup_data = my_Apriori(dataset, target_item, min_support=2)  # 最小支持度设置为2
    print(f'my_Apriori:{time.time() - t:.12f}s')

    print("具有关联的商品是{}".format(my_F))  # 带变量的字符串输出,必须为字典符号表示
    print("对应的支持度为{}".format(my_sup_data))
```

## 实验

### 数据集

通过定义生成dataset的函数，生成40条数据，每条数据包含商品数量为1-10，商品的编号为1-50，5次实验生成如下数据集

dataset1:

```python
dataset = [[9, 45, 14], [37, 15, 17, 25], [30, 14, 31], [38, 16, 22, 37, 35, 19], [44, 23, 19, 5, 19], [26, 41, 26, 20, 33, 26], [17, 34, 18, 23, 9, 11, 17, 13, 28], [3, 29, 45, 46, 17, 16, 9], [40, 48, 16, 18, 15, 43, 20, 20, 15, 16], [26, 22, 6, 42], [19, 2, 42, 20, 17, 24, 41, 30, 22, 24], [17], [29, 28, 25, 40, 22, 47, 44, 10, 35, 38], [1, 39, 9], [19, 50, 27, 30, 48, 18, 36, 9], [20], [39], [50, 14, 28, 3, 11, 12], [41, 26], [46, 49, 3, 8, 9, 45, 41, 13, 35, 21], [25, 3, 34, 48, 25, 24, 11, 35, 18], [3, 41, 33, 6, 25, 42, 33, 27], [43, 28, 8, 47, 23], [25, 43, 36, 13, 10], [36, 35, 8, 29, 19, 29, 27, 15, 42, 5], [18, 37, 24, 17], [42, 9], [40], [47, 34, 38, 21, 13, 5, 42], [23], [18, 32, 16, 48, 32, 13, 14, 40, 34], [41, 2, 44, 7, 41, 45, 36, 46], [33, 14], [27, 7, 26, 22], [19, 4, 4, 15, 23, 41, 24, 9, 42, 27], [10, 40, 43, 3, 23, 33, 23, 46, 38], [37, 28, 6, 32, 12, 13], [19, 4, 17, 15, 47, 15], [43, 23, 36, 38, 4, 27, 47, 8, 25], [31, 35, 26, 19, 39, 13, 47]]
```

dataset2:

```python
dataset = [[47, 3, 3, 26, 19, 21, 18, 11, 45, 22], [46, 46, 27, 48, 26, 30], [3, 9, 25, 49, 23], [43], [23, 22, 43, 21, 36, 10, 40, 21], [19, 50, 26, 15, 37, 9, 43], [36, 44, 25], [45, 33, 20, 8, 28, 36], [48, 13], [35, 39, 15, 33, 22, 12], [14, 40, 41, 28, 32, 35, 17, 8, 7, 18], [16, 13, 28, 28, 42, 22, 31, 29, 7], [44, 37, 9, 47, 23, 18, 38, 7, 9, 8], [39], [48, 34, 9], [20, 31, 21, 8, 5], [39, 27, 25, 10, 12], [40, 16, 12, 31, 30, 50, 2], [19, 46, 45], [37, 33, 21, 3, 37], [43], [3, 40, 25, 20, 23, 24, 34], [7, 41, 38, 31, 26], [40, 3, 30], [14, 47, 5, 5, 19, 38, 35, 22], [24, 4, 44, 46, 32], [30], [3], [30, 45, 16, 19, 20, 38, 10, 36], [27, 28], [42], [24, 6, 50, 7, 8, 48, 36, 27, 50, 50], [39], [11, 44, 8, 2, 41, 46, 28, 21, 16, 7], [10, 13, 26, 20, 8], [37, 27, 12, 34, 37], [10, 36, 31, 3, 2, 38, 37, 36], [30], [18, 20, 46, 11, 13, 23, 5], [30, 6, 48, 17, 47, 1]]
```

dataset3:

```python
dataset = [[28, 37, 20, 2, 12, 50, 20], [37, 45, 49, 18, 30], [36, 10, 35, 21, 38, 30, 39, 30, 17], [13, 47, 47, 18, 48, 38, 45, 30], [23, 37, 10, 3, 50, 31, 10, 22, 7], [46], [35, 30, 3, 17, 38, 12, 37, 33, 29], [29, 40, 22], [27, 48, 8, 17, 11, 15, 45, 47], [50, 1, 16, 42, 23, 24], [27, 49, 10, 20, 16, 29, 12, 47, 12, 12], [42, 42, 24, 25, 13, 8, 44], [43, 2, 14, 31], [49, 45], [22, 33, 2, 31, 42, 12], [39, 7, 40, 37, 3, 12, 31, 32, 26], [2, 8, 40, 18, 24, 21, 8, 34], [8, 18, 19, 29, 29, 9, 20, 48, 20, 19], [15, 9, 30, 17, 39, 22, 32], [9, 28, 12, 30, 19], [29, 44, 47], [40, 37, 34, 36, 26, 26, 47, 28, 38, 18], [5, 18, 34], [12, 43, 41, 21, 31, 21, 20, 40], [42, 40, 14], [1, 41, 47, 30, 45, 41, 24], [17, 26, 33, 42, 50, 35, 22, 48], [38, 31, 38, 47, 42, 19, 19, 7, 37], [44, 12, 1, 48, 15], [7, 5, 46, 13, 27], [44, 44, 28, 25, 18, 3, 23, 40], [48, 20, 29, 24, 49, 46], [50, 4, 39, 9, 17, 12, 42, 40, 28, 27], [19, 13, 38, 22, 1, 20, 17, 43, 4], [12, 27, 39, 32, 38, 13, 43], [37, 4, 6, 21, 48, 9], [30], [44, 46, 21, 21, 16, 28, 7, 3], [13, 17, 46, 30, 48, 42], [28, 37, 43, 22, 32, 47, 27, 45, 7, 41]]
```

dataset4:

```python
dataset = [[16, 7, 40], [31], [28, 17], [27, 37, 10, 40, 45, 8, 43, 23], [18, 41, 12], [13, 7, 10, 3], [27, 16, 42, 43, 36, 27, 22, 24, 19, 30], [11, 49], [49, 42, 27, 47, 17, 43, 37], [11, 9, 49, 32, 23, 2, 10], [16], [42, 30, 13, 43, 24], [5, 24, 18], [21, 27, 17], [24, 23, 17, 5, 6, 29, 25, 43], [36, 17, 20, 25, 5, 2, 12, 29, 50, 3], [7, 18, 43], [17, 32, 36, 47], [33, 7, 17, 50, 3, 17, 17, 24, 11, 8], [14, 43], [11, 31, 12, 43, 50, 36, 22, 49, 41], [10, 2], [1], [3, 28, 34, 31, 49], [14, 4, 7, 42, 28, 18, 29, 20], [9, 6, 9, 29, 34, 4, 33], [19, 46, 31, 50, 23, 6, 21, 41, 20, 4], [43], [35, 45, 7, 9, 28], [46, 20, 17, 30, 4, 33, 4, 19, 49, 22], [5, 1, 34, 28, 39, 12, 3, 15], [35, 24, 27, 18, 40, 20, 23, 36, 46, 46], [16, 8, 45, 35, 9, 35], [10, 24], [41, 33, 9], [43, 18, 4, 19, 4, 20], [10, 22, 50, 3, 12, 42, 31, 27], [15, 12, 8, 19, 37, 44, 49], [11, 6, 6, 7, 32], [2, 37, 5, 36, 49, 43, 24, 49]]
```

dataset5:

```python
dataset = [[46, 7], [5, 15], [13, 19, 10, 37, 14, 42, 22, 29, 15, 46], [28, 21, 11, 44, 49, 29, 10, 35, 25, 22], [25, 34, 36], [5, 34, 13, 35, 4, 12, 3, 34, 37], [31, 20, 36, 41, 5, 12], [49, 16, 24, 22, 15, 15, 13], [37, 9, 11, 30, 46, 22], [27, 40, 42, 18, 13, 3], [44], [21, 6, 3], [9, 17, 24], [28, 31, 18, 34, 17, 15, 36, 4, 34, 12], [28, 49, 48, 39, 11, 47, 3, 45, 19, 44], [45], [31, 11, 3, 24, 9, 9, 3], [36, 21, 3, 24, 26, 6, 36, 8], [45, 6, 11, 12, 11, 45, 22, 19, 2, 19], [25, 42, 19, 34, 9, 5, 5, 40, 41, 33], [40, 50, 50, 5, 45, 2, 47, 13], [18, 26, 11, 46, 49], [4, 36, 28, 22, 36, 38], [10, 39, 39, 3, 11, 37, 8], [5, 16, 17, 35, 50, 4], [46, 32, 41, 34, 6, 23, 45], [40, 47, 31, 42, 42, 33, 40, 20, 21], [4, 16, 11, 50, 22, 18], [45, 14, 50, 45, 23, 12, 28], [14, 36, 16, 7, 49, 16], [3, 34, 26, 47, 9, 19, 49, 25, 34, 46], [9, 48, 7, 33, 43, 22, 14, 39], [23, 42, 18, 21, 41, 40, 23, 28], [39, 34], [30, 14, 13, 45, 21], [34, 19, 21, 20, 36, 33, 33], [29, 15, 39, 7, 36, 16, 45, 40, 18], [2, 9, 26, 37, 7, 27, 11], [39], [48, 11, 2, 31]]
```



### 实验结果的对比

将apriori算法运行后逐个对每个频繁项集进行目标项是否在其中的检验，然后得出与目标项相关的频繁项集，再与改进后的算法的结果进行比对，从结果可知改进算法的求解与apriori算法一致。

在预先设定target_item为2的情况下对同一个数据集Apriori和single_Apriori运行耗时和最终的对应支持度如下：

|            | 耗时(s)     | 耗时(s)        | 与目标集具有关联的商品                             | 与目标集具有关联的商品                             |                                                              |
| ---------- | ----------- | -------------- | -------------------------------------------------- | -------------------------------------------------- | ------------------------------------------------------------ |
| 数据集序号 | Apriori     | single_Apriori | Apriori                                            | single_Apriori                                     | 对应的支持度                                                 |
| 1          | 0.925539017 | 0.002989769    | (2), (41, 2)                                       | (2), (41, 2)                                       | {(2,): 2, (41, 2): 2}                                        |
| 2          | 0.490314722 | 0.001993179    | (2), (16, 2), (2, 31)                              | (2), (16, 2), (2, 31)                              | {(2,):  3, (16, 2): 2, (2, 31): 2}                           |
| 3          | 0.896590471 | 0.002990246    | (2), (2, 12), (2, 31)                              | (2), (2, 12), (2, 31)                              | {(2,): 4, (2, 12): 2, (2, 31): 2}                            |
| 4          | 0.63056922  | 0.001994848    | (2), (2, 5), (2, 10), (2, 36), (49, 2), (2, 36, 5) | (2), (2, 5), (2, 10), (2, 36), (49, 2), (2, 36, 5) | {(2,): 4, (2, 5): 2, (2, 10): 2, (2, 36): 2, (49, 2): 2, (2, 36, 5): 2} |
| 5          | 0.662516356 | 0.002990007    | (2), (2, 11), (2, 45)                              | (2), (2, 11), (2, 45)                              | {(2,): 4, (2, 11): 3, (2, 45): 2}                            |



















整理得到图表如下：

![P10](D:\Typora\notes\2023_2024B\数据挖掘与数据仓库\pics\P10.png)

结合图表可知，对单一特定项的频繁项集进行求解时，两种算法之间的求解效率在指定数据集上的数量级差距是$10^3$左右，所以在对单一特定项的频繁项集进行求解改进方案single_Apriori更优。

## 其它的可行方案

### 算法介绍

**FP-growth：**

假设原数据集为：

| TID  | itemset list   |
| ---- | -------------- |
| T100 | I2, I1, I5     |
| T200 | I2, I4         |
| T300 | I2, I3         |
| T400 | I2, I1, I4     |
| T500 | I1, I3         |
| T600 | I2, I3         |
| T700 | I1, I3         |
| T800 | I2, I1, I3, I5 |
| T900 | I2, I1, I3     |

频繁模式树建立如下，假设只对I5进行求解：

![P11](D:\Typora\notes\2023_2024B\数据挖掘与数据仓库\pics\P11.png) 

首先进行统计和基于最小支持度的项的裁剪。比如对于I5的子事务集（也就是条件模式基），需要先统计，得到{I2 : 2, I1 : 2, I3 : 1}（从这些节点到I5的路径，经过的节点数量）。设置最小支持度为2，所以我们需要去除count只为1的I3（不频繁的项集超集也一定不频繁），然后排序得到I2, I1，剩下的事务集重构为{I2, I1 : 1}, {I2, I1 : 1}，很明显这两个前缀可以合并为{I2, I1 : 2}，然后画出这个裁剪后的子事务集的频繁模式树：

<img src="D:\Typora\notes\2023_2024B\数据挖掘与数据仓库\pics\P12.png" width = "20%" height = "20%" alt="图片名称" align=center />  

然后找出这棵频繁模式树的所有路径，可以看到上面的这棵频繁模式树只有一条路径，然后找出这条路径上节点所有可能的组合，也就是{I2, I1}, {I2}, {I1}，这也就是{I2, I1}的幂集。然后对于每一个组合类型，再加上I5，于是就得到了这条路径上得到的频繁项集：{I2, I1, I5}, {I2, I5}, {I1, I5}。所有路径上的频繁项集合并起来就是I5对应的频繁项集。

### 优劣比较

但是FP-growth方法在一开始就将整个数据集扫描了一遍，这样的初始开销是较single_apriori大的。但是后续针对单个项的查找复杂度可以缩减到$O(m)$，m为生成的频繁模式树的叶子节点数量。在已经生成了频繁模式树后，FP-growth对单个商品所关联的频繁项集挖掘性能可能较single-apriori提升。

## 参考文献

apriori: [rj.dvi (philippe-fournier-viger.com)](https://www.philippe-fournier-viger.com/spmf/apriori_longer.pdf)

