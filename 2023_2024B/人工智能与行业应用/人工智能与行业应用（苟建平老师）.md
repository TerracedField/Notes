# So-Vits音色克隆模型研究综述

班级：软件工程（普通）2班

学号：222021321262043

姓名：杨锦烨

目录：

[toc]

## 摘要

本文综述了So-Vits音色克隆模型的研究进展。So-Vits模型基于VITS（Conditional Variational Autoencoder with Adversarial Learning for End-to-End Text-to-Speech）框架，通过改进解码器和输入方式，实现了更高质量的歌声转换。文章首先介绍了语音转换技术的发展历程，然后详细解析了So-Vits模型的原理和结构，包括数据预处理、特征提取和推理阶段。最后，本文还探讨了So-Vits在实际应用中的潜力和未来的研究方向。

## 关键词

So-Vits；音色克隆；语音转换；歌声转换；深度学习

## 引言

语音转换技术作为人工智能领域的一个重要分支，其目标是改变源说话者的语音特征，使其听起来像目标说话者的声音，同时保留语言内容和说话者的身份。传统的语音转换方法主要依赖于信号处理技术，但这些方法存在局限性，无法生成指定说话人的音色。随着深度学习技术的发展，特别是变分自编码器（VAE）和生成对抗网络（GAN）的应用，语音转换技术取得了显著的进步。

So-Vits模型作为VITS的一个变种，通过引入NSF-HiFiGAN解码器和SoftVC的输入方式，进一步提升了歌声转换的性能。本文将对So-Vits模型进行深入分析，探讨其在语音转换领域的应用前景，并提出可能的改进方向。

## 综述

### So-Vits原理解析

#### Vits

VITS: Conditional Variational Autoencoder with Adversarial Learning for End-to-End Text-to-Speech

语音转换，也被称为声音转换或语音变换，是一种用于改变人的语音特征的技术，同时保留语言内容和说话者的身份。它涉及修改源说话者的声音特性，使其听起来像目标说话者的声音。语音转换的目标是将源说话者的声音的各个方面进行转换，如基频（音高）、频谱包络（声道特征）和韵律特征（节奏、语调和重音），以使其与目标说话者的特征相匹配。这样，源说话者的声音就能够采用目标说话者的特征，而不改变语言内容。

最开始人们通过信号处理的方式进行语音转换，主要是通过基音和速度对语音信息进行建模，然后通过PSOLA对语音基音或者速度进行修改从而达到变声的效果。但是这种修改只能简单地调高/调低音调，并不能达到生成指定说话人音色的目的。随着深度学习的兴起，许多人开始尝试使用神经网络强大的特征提取能力对语音信息进行建模。早期的语音转换分为三个部分，分别是语音分析、特征映射和语音重建。随着韩国科学院在ICML 2021发布VITS(VITS: Conditional Variational Autoencoder with Adversarial Learning for End-to-End Text-to-Speech)论文和代码，语音转换进入了端到端的时代。VITS本身是用于语音合成任务，但语音合成和语音转换具有相似的结构，或者可以说语音转换网络是利用了语言信息的语音合成网络。

![P1](D:\Typora\notes\2023_2024B\人工智能与行业应用\pics\P11.jpg)

VITS的主体是一个条件VAE模型，包含了prior encoder，posterior encoder，decoder，discriminator和stochastic duration predictor。其中posterior encoder和discriminator只在训练中使用。VITS采用了和Fastspeech系统相同的transformer作为文本Encoder，与Glow-TTS相同的Flow结构作为VAE的主体以及和HiFiGAN生成器相同的反卷积作为Decoder，此外还采用Glow-TTS相同的单调对齐搜索算法(MAS), 保证生成对齐的稳定性。在推理时，输入的phoneme透过prior encoder得到隐变量z，然后z通过decoder生成原始波形。VITS整体训练和推理流程如下所示：

![P1](D:\Typora\notes\2023_2024B\人工智能与行业应用\pics\P12.jpg)

#### Singing Voice Conversiond

此处以SoVITS[9]为例介绍如何使用VITS进行歌声转换。SoVITS是在VITS的基础上，将解码器改为NSF-HiFiGAN，输入改为ContentVec，在推理时对ContentVec预测和pitch decoder的输出进行聚类，以减少输入语音泄漏，并在原始网络之前增加了Conv1d。

SoVITS输入改自SoftVC，原始SoftVC论文中的encoder有两种，如下图所示，其中第一种是离散内容编码器，它首先提取特征，然后通过聚类的方法生成一系列离散的语音单元；另一种是软内容编码器，它不使用聚类的方法而是训练一个网络去预测离线语音单元，去除了不必要的说话人信息，因此更适合语音转换这个任务，这也是SoVITS中采用soft content encoder的原因。

![P1](D:\Typora\notes\2023_2024B\人工智能与行业应用\pics\P13.jpg)

此外，由于是歌声转换任务，从作者代码中容易发现作者将f0显式的引入到了vits中以便更好的对歌声信息进行建模。最后作者将声码器改为NSF-HiFiGAN以解决声音中断的问题。在推理过程中，只需说话人信息和音频信息，其中音频信息会首先计算f0，然后将说话人信号，音频信号和f0一起送到encoder，随后encoder的输出经过flow和NSF-HiFiGAN生成最终的结果，其完整流程如下所示：

![P1](D:\Typora\notes\2023_2024B\人工智能与行业应用\pics\P14.jpg)

### So-Vits项目解析

项目解析采用倒序的方式，先从推理过程开始介绍人声克隆的工作流，以及各个模型的功能，之后再去详细介绍各个模型。此部分主要聚焦于整个项目中对音频数据的预处理、特征提取和最终的推理过程（音频生成的过程）。

#### 数据预处理阶段

预处理阶段主要有三种数据：

- source audio : 是指我们要转换的源音频
- Vocals.wav : 要通过人声分离软件，过滤出源音频中的纯人声
- Vocals.wav 44k : 模型只能处理采样率为44k的音频，所以对人声音频进行重采样

三种数据通过如下的步骤进行处理，最后得到44k的干声：

![P1](D:\Typora\notes\2023_2024B\人工智能与行业应用\pics\P1.png)

#### 特征提取阶段

对源人声的特征提取主要分为以下5类：

1. 预训练模型，通过预训练模型对音频进行特征提取
2. 乐理特征，基音频率f0，高音的频率
3. 频域特征，频谱/梅尔频谱：mel spectrogram
4. 能量特征，音频音量: audio volume

提取特征的流程图如下，其中$audio_{44k}$是上一阶段得到的44k人声干声

![P2](D:\Typora\notes\2023_2024B\人工智能与行业应用\pics\P2.png)

##### 预训练特征提取

预训练模型主要有三种。其中不同模型的特征表示维度不一样，如果显存较小可以选择低维的特征编码，Contextvec可以选择不同隐藏层的特征表示。

- Hubert : 基于BERT改进的HuBERT使用BERT的架构，并做了修改以适应语音序列。HuBERT可以学习出高质量的语音表示，在多种下游任务上都取得了state-of-the-art的结果，如语音情感识别、语言识别、语音生成与理解和人声克隆等。
- Whisper：是openai提出利用大规模弱监督学习实现了端到端语音识别模型，在少量标注数据的条件下，可以达到很好的识别性能，同时也学习到了语音的高质量表示。其创新之处在于设计的弱监督学习框架，这为利用海量无标注数据提高语音识别系统的鲁棒性提供了一个值得借鉴的范例。
- Contextvec : 一种新的语音自监督表示学习方法，通过Hubert生成训练标签，通过学生网络来学习音频的表征，其在人声验证和语音理解等任务上达到SOTA的性能，是一种非常有效的语音表示学习框架

工作流程图如下：

![P3](D:\Typora\notes\2023_2024B\人工智能与行业应用\pics\P3.png) 

##### 乐理特征提取

So-Vits提供了四种不同的f0计算方法，其中`uv`是基于f0的衍生特征，计算方式如下：

```python
data = np.reshape(f0, (f0.size, 1))
    
vuv_vector = np.zeros((data.size, 1), dtype=np.float32)
vuv_vector[data > 0.0] = 1.0
vuv_vector[data <= 0.0] = 0.0
```

其工作流程图如下：

![P4](D:\Typora\notes\2023_2024B\人工智能与行业应用\pics\P4.png)

##### 频域特征提取

频域特征提取的工作流程图如下：

![P5](D:\Typora\notes\2023_2024B\人工智能与行业应用\pics\P5.png)

对其中两个重要参数的解释如下：

- spectrogram : 通过对人声音频的时域信号进行短时傅里叶变换（STFT）得到。
- mel spectrogram : 频谱经过梅尔滤波后得到梅尔频谱。

##### 能量特征

工作流程图：

![P5](D:\Typora\notes\2023_2024B\人工智能与行业应用\pics\P6.png)

关键参数aug volume、aug mel：是在音频数据上增加了随机噪声后，在进行特征提取得到的增强特征。

#### 推理阶段

So-Vits在推理阶段提供了三种方案来生成目标音频，最后还可以选择是否采用预训练模型`NSF_HifiGAN`对结果进行增强修饰。

1. 纯生成，输入源音频特征到生成器生成目标音频。
2. 纯扩散，输入源音频特征到扩散模型生成目标音频。
3. 生成扩散，在生成模型结果的基础上，再次计算梅尔频谱，输入扩散模型，生成目标音频。

流程图如下：

![P5](D:\Typora\notes\2023_2024B\人工智能与行业应用\pics\P7.png)

##### 生成模型

生成模型主要用到预训练训练模型的特征编码和乐理特征，在生成音频同时，也预测乐理特征。

流程图如下：

![P5](D:\Typora\notes\2023_2024B\人工智能与行业应用\pics\P8.png)

##### 扩散模型

扩散模型基本全部用到之前的特征，扩散模型的预测不直接生成目标音频数据，而是生成目标音频的梅尔频谱，然后通过转换得到最终的目标音频。

流程图如下：

![P5](D:\Typora\notes\2023_2024B\人工智能与行业应用\pics\P9.png)

##### 生成扩散模型

生成扩散模型分为生成阶段与扩散阶段：

- 生成阶段：与生成模型一致
- 扩散阶段：音频的编码特征和能量特征仍采用源音频提取的，乐理特征和梅尔频谱分别从生成器和生成音频中得到。

##### 增强阶段

增强阶段需要使用HiFiGAN，HiFiGAN是较为常用的声码器，能够将声学模型产生的频谱转换为高质量的音频，从名字中的GAN可以看出采用的是生成对抗网络模型。

![P5](D:\Typora\notes\2023_2024B\人工智能与行业应用\pics\P10.png)

## 结果展示

在探索So-Vits模型的应用潜力和实际效果方面，以下三个视频链接展示了使用So-Vits训练得到的成果，它们分别涵盖了不同风格的歌声转换，体现了So-Vits技术在音色克隆方面的多样性和灵活性：

1. **AI 孙燕姿《爱在西元前》** https://www.bilibili.com/video/BV1io4y1w73k- 该视频演示了So-Vits模型如何将孙燕姿的歌声风格应用到一首具有历史背景的歌曲上。汉谟拉比法典的古老旋律与现代歌声的结合，展示了So-Vits在处理不同文化元素融合时的能力。
2. **AI 孙燕姿 × 周杰伦《暗号》合唱版** https://www.bilibili.com/video/BV1Dv4y1E7UA - 此视频呈现了So-Vits在实现两位著名歌手声音融合方面的成果。孙燕姿和周杰伦的独特声音风格在合唱中得到了和谐统一，证明了So-Vits在处理复杂声音交互时的高效性。
3. **AI 孙燕姿《勇气》** https://www.bilibili.com/video/BV1gp42127XC - 这首歌通过So-Vits模型的转换，展现了孙燕姿声音的温暖和力量。歌声中的情感表达丰富，传递出面对困难时所需的勇气和决心，体现了So-Vits在捕捉和再现歌声情感深度方面的先进技术。

这些视频不仅展示了So-Vits模型在音色克隆上的技术实力，也为广大听众提供了全新的听觉体验。通过这些成果，我们可以看到So-Vits在未来音乐制作、虚拟偶像开发以及语音技术应用方面的巨大潜力。

##  总结与展望

**总结**

本文对So-Vits音色克隆模型进行了全面的综述。从语音转换技术的发展历程到So-Vits模型的详细解析，本文深入探讨了该模型在音色克隆领域的应用和优势。So-Vits模型通过结合VITS的先进架构和NSF-HiFiGAN解码器，实现了高质量的歌声转换。在数据预处理、特征提取和推理阶段，So-Vits展示了其在处理复杂语音信号方面的强大能力。通过使用不同的预训练模型和特征提取技术，So-Vits能够生成具有高度自然性和可定制性的语音输出。

**展望**

尽管So-Vits模型在音色克隆方面取得了显著的成果，但仍存在一些挑战和改进空间。未来的研究可以从以下几个方面进行：

1. **模型泛化能力**：提高模型对不同说话者和不同语料的泛化能力，使其能够适应更广泛的应用场景。
2. **实时性能优化**：优化模型的推理速度，以满足实时语音转换的需求。
3. **情感和语调的表达**：增强模型对语音中情感和语调变化的捕捉能力，使生成的语音更加丰富和自然。
4. **多语言支持**：扩展模型对多语言的支持，以覆盖更广泛的用户群体。
5. **用户交互性**：开发更加用户友好的界面，使非专业人士也能轻松使用So-Vits进行音色克隆。
6. **伦理和隐私问题**：随着技术的发展，需要深入探讨音色克隆技术在伦理和隐私方面的影响，并制定相应的指导原则。

## 参考文献

1. 论文原文：Conditional Variational Autoencoder with Adversarial Learning for End-to-End Text-to-Speech（[Jaehyeon Kim](https://arxiv.org/search/cs?searchtype=author&query=Kim,+J), [Jungil Kong](https://arxiv.org/search/cs?searchtype=author&query=Kong,+J), [Juhee Son](https://arxiv.org/search/cs?searchtype=author&query=Son,+J)，ICML 2021）：https://arxiv.org/abs/2106.06103
1. 技术博客：VITS论文阅读：https://blog.csdn.net/zzfive/article/details/127061469
1. 开源项目地址：sovits: vocal generation network：https://github.com/cjplol/sovits/tree/4.0
