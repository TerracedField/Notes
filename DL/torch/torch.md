[toc]

## tensor

### catåˆå¹¶

```
(tensor([[ 0.4755,  0.1765,  0.2200, -0.3758,  0.0590, -0.1009, -0.2270,  0.3249],
         [ 0.4501,  0.1816,  0.3269, -0.3423,  0.1115,  0.0378, -0.2091,  0.1768],
         [ 0.3281,  0.0851,  0.2253, -0.4032,  0.1696, -0.0475, -0.1658,  0.3093]],
        grad_fn=<AddmmBackward0>),
 tensor([[-0.2055,  0.0579, -0.2525,  0.0588],
         [-0.1978,  0.0563, -0.2532,  0.0682],
         [-0.1921,  0.0464, -0.2399,  0.0697]], grad_fn=<AddmmBackward0>))
```

```python
torch.cat((net1(X1), net2(X2)), dim=1)
```

```
tensor([[-0.0395,  0.2852, -0.1403,  0.1256, -0.2409, -0.4229,  0.2372,  0.2719,
          0.0626, -0.1902,  0.0975, -0.1365],
        [-0.1373,  0.2139,  0.0534,  0.0018, -0.2146, -0.2590,  0.2603,  0.4758,
          0.0848, -0.2055,  0.1218, -0.1293],
        [-0.0796,  0.2327, -0.0647,  0.1997, -0.1470, -0.3759,  0.2663,  0.3674,
          0.0888, -0.1786,  0.1104, -0.1245]], grad_fn=<CatBackward0>)
```

æŒ‰ç¬¬ä¸€ç»´åˆå¹¶

### ç»´åº¦å˜æ¢

æ•°å­—å¯¹åº”åŸæ¥çš„ç»´åº¦ä½ç½®

```python
# åŸæ¥ E_i.shape:(N,C,H,W)
E_i = E_i.permute(2, 3, 1, 0) # æ¢æˆ(H,W,C,N)
```

äº¤æ¢0,1ä¸¤ä¸ªç»´åº¦çš„æ•°æ®


```python
P = P.transpose(0, 1)
```

### å»é™¤ç»´åº¦

å»é™¤å€’æ•°ç¬¬ä¸€ç»´åº¦

```python
P = P.squeeze(-1)
```

å»é™¤å€’æ•°ç¬¬ä¸€ï¼ŒäºŒç»´åº¦

```python
P = P.squeeze(-1,2)
```

### åˆå¹¶ç»´åº¦

```python
# å‡è®¾E_iå·²ç»å®šä¹‰ï¼Œå½¢çŠ¶ä¸º[3, 64, 128, 256]
E_i = torch.randn(3, 64, 128, 256)

# ä½¿ç”¨torch.viewåˆå¹¶åä¸¤ä¸ªç»´åº¦
E_i_reshaped = E_i.view(3, 64, -1)  # -1è¡¨ç¤ºè‡ªåŠ¨è®¡ç®—è¯¥ç»´åº¦çš„å¤§å°
```

æˆ–è€…reshapeåˆå¹¶ä¸­é—´ä¸¤ç»´åº¦ï¼š

```python
x = torch.reshape(x, (x.shape[0], -1, x.shape[3]))
```

### å¢åŠ ç»´åº¦

å‡è®¾ï¼š

```
Q_i.shape: torch.Size([3, 64, 32768])
```

å¯¹äºæä¾›çš„å¼ é‡`Q_i`çš„å½¢çŠ¶`torch.Size([3, 64, 32768])`ï¼Œå¦‚æœæƒ³åœ¨ç¬¬ä¸€ä¸ªç»´åº¦å‰æ·»åŠ ä¸€ä¸ªæ–°çš„ç»´åº¦ï¼Œå¯ä»¥ä½¿ç”¨ä»¥ä¸‹ä»£ç ï¼š

```python
Q_i = Q_i.unsqueeze(0)
```

è¿™å°†åœ¨ç´¢å¼•0çš„ä½ç½®æ·»åŠ ä¸€ä¸ªæ–°çš„ç»´åº¦ï¼Œç»“æœå½¢çŠ¶å°†æ˜¯`torch.Size([1, 3, 64, 32768])`ã€‚

å¦‚æœæƒ³åœ¨æœ€åä¸€ä¸ªç»´åº¦åæ·»åŠ ä¸€ä¸ªæ–°çš„ç»´åº¦ï¼Œå¯ä»¥ä½¿ç”¨ï¼š

```python
Q_i = Q_i.unsqueeze(-1)
```

è¿™å°†åœ¨æœ€åä¸€ä¸ªç»´åº¦åæ·»åŠ ä¸€ä¸ªæ–°çš„ç»´åº¦ï¼Œç»“æœå½¢çŠ¶å°†æ˜¯`torch.Size([3, 64, 32768, 1])`ã€‚

### rand

åœ¨PyTorchä¸­ï¼Œ`torch.rand`å’Œ`torch.randn`æ˜¯ä¸¤ä¸ªç”¨äºç”Ÿæˆéšæœºæ•°çš„å‡½æ•°ï¼Œå®ƒä»¬çš„ä¸»è¦åŒºåˆ«åœ¨äºç”Ÿæˆçš„éšæœºæ•°åˆ†å¸ƒä¸åŒï¼š

1. **torch.rand**ï¼š

	- è¿™ä¸ªå‡½æ•°ç”¨äºç”Ÿæˆå‡åŒ€åˆ†å¸ƒçš„éšæœºæ•°ã€‚

	- å®ƒç”Ÿæˆçš„éšæœºæ•°åœ¨[0, 1)åŒºé—´å†…ï¼Œå³ä»0ï¼ˆåŒ…æ‹¬ï¼‰åˆ°1ï¼ˆä¸åŒ…æ‹¬ï¼‰ä¹‹é—´çš„éšæœºæµ®ç‚¹æ•°ã€‚

	- ç”¨æ³•ç¤ºä¾‹ï¼š

		```python
		import torch
		tensor = torch.rand(size)  # sizeå¯ä»¥æ˜¯æ•´æ•°ï¼Œä¹Ÿå¯ä»¥æ˜¯æ•´æ•°çš„å…ƒç»„
		```

		è¿™å°†ç”Ÿæˆä¸€ä¸ªå½¢çŠ¶ä¸º`size`çš„å¼ é‡ï¼Œå…¶ä¸­çš„å…ƒç´ æ˜¯ä»[0, 1)å‡åŒ€åˆ†å¸ƒçš„éšæœºæ•°ã€‚

2. **torch.randn**ï¼š

	- è¿™ä¸ªå‡½æ•°ç”¨äºç”Ÿæˆæ ‡å‡†æ­£æ€åˆ†å¸ƒï¼ˆå‡å€¼ä¸º0ï¼Œæ ‡å‡†å·®ä¸º1ï¼‰çš„éšæœºæ•°ã€‚

	- å®ƒç”Ÿæˆçš„éšæœºæ•°éµå¾ªæ­£æ€åˆ†å¸ƒï¼Œå³å¤§éƒ¨åˆ†æ•°å€¼é›†ä¸­åœ¨0é™„è¿‘ï¼Œåˆ†å¸ƒçš„å½¢çŠ¶å‘ˆé’Ÿå½¢æ›²çº¿ã€‚

	- ç”¨æ³•ç¤ºä¾‹ï¼š

		```python
		import torch
		tensor = torch.randn(size)  # sizeåŒæ ·å¯ä»¥æ˜¯æ•´æ•°ï¼Œä¹Ÿå¯ä»¥æ˜¯æ•´æ•°çš„å…ƒç»„
		```

		è¿™å°†ç”Ÿæˆä¸€ä¸ªå½¢çŠ¶ä¸º`size`çš„å¼ é‡ï¼Œå…¶ä¸­çš„å…ƒç´ æ˜¯æ ‡å‡†æ­£æ€åˆ†å¸ƒçš„éšæœºæ•°ã€‚

æ ¹æ®ä½ çš„åº”ç”¨åœºæ™¯ï¼Œä½ å¯ä»¥é€‰æ‹©ä½¿ç”¨`torch.rand`æˆ–`torch.randn`ã€‚ä¾‹å¦‚ï¼Œåœ¨åˆå§‹åŒ–ç¥ç»ç½‘ç»œçš„æƒé‡æ—¶ï¼Œé€šå¸¸ä½¿ç”¨`torch.randn`æ¥æ¨¡æ‹Ÿæƒé‡çš„åˆå§‹åˆ†å¸ƒï¼Œå› ä¸ºæ­£æ€åˆ†å¸ƒå¯ä»¥å¸®åŠ©æ¢¯åº¦ä¸‹é™ç®—æ³•æ›´å¿«åœ°æ”¶æ•›ã€‚è€Œåœ¨éœ€è¦å‡åŒ€åˆ†å¸ƒçš„éšæœºæ•°æ—¶ï¼Œæ¯”å¦‚åœ¨æŸäº›é‡‡æ ·ç®—æ³•ä¸­ï¼Œå¯èƒ½ä¼šä½¿ç”¨`torch.rand`ã€‚

### rearrange

`rearrange` å‡½æ•°æ˜¯ `einops` åº“ä¸­çš„ä¸€ä¸ªéå¸¸å¼ºå¤§çš„å‡½æ•°ï¼Œå®ƒç”¨äºé‡æ–°æ’åˆ—å¤šç»´æ•°ç»„çš„å½¢çŠ¶å’Œç»´åº¦ã€‚è¿™ä¸ªå‡½æ•°çš„ä¸€èˆ¬å½¢å¼æ˜¯ï¼š

```python
rearrange(input, pattern, **kwargs)
```

- `input`ï¼šè¦é‡æ–°æ’åˆ—çš„è¾“å…¥æ•°ç»„ã€‚
- `pattern`ï¼šä¸€ä¸ªå­—ç¬¦ä¸²ï¼ŒæŒ‡å®šäº†è¾“å…¥æ•°ç»„çš„ç»´åº¦å¦‚ä½•è¢«é‡æ–°æ’åˆ—ã€‚
- `**kwargs`ï¼šä¸€äº›é¢å¤–çš„å‚æ•°ï¼Œç”¨äºæ›¿æ¢æ¨¡å¼å­—ç¬¦ä¸²ä¸­çš„å˜é‡ã€‚

åœ¨ä½ æä¾›çš„ä»£ç ä¸­ï¼Œ`rearrange` å‡½æ•°è¢«ç”¨æ¥å°†å›¾åƒçš„äºŒç»´ç©ºé—´ç‰¹å¾æ˜ å°„ï¼ˆå³å›¾åƒçš„åƒç´ ï¼‰è½¬æ¢ä¸ºä¸€ç³»åˆ—ä¸€ç»´çš„å›¾åƒå—ï¼ˆpatchesï¼‰ï¼Œè¿™äº›å›¾åƒå—å¯ä»¥è¢«é€å…¥Transformeræ¨¡å‹ä¸­è¿›è¡Œå¤„ç†ã€‚

å…·ä½“æ¥è¯´ï¼Œ`rearrange` å‡½æ•°çš„è°ƒç”¨å¦‚ä¸‹ï¼š

```python
img_patches = rearrange(x,
                        'b c (patch_x x) (patch_y y) -> b (x y) (patch_x patch_y c)',
                        patch_x=self.patch_size, patch_y=self.patch_size)
```

è¿™é‡Œçš„å‚æ•°è§£é‡Šå¦‚ä¸‹ï¼š

- `x`ï¼šè¾“å…¥æ•°ç»„ï¼Œé€šå¸¸æ˜¯å›¾åƒçš„å¼ é‡ï¼Œå½¢çŠ¶ä¸º `(batch_size, channels, height, width)`ã€‚
- `'b c (patch_x x) (patch_y y)'`ï¼šè¿™æ˜¯è¾“å…¥æ¨¡å¼å­—ç¬¦ä¸²ï¼Œå…¶ä¸­ï¼š
  - `b` ä»£è¡¨æ‰¹æ¬¡å¤§å°ï¼ˆbatch sizeï¼‰ã€‚
  - `c` ä»£è¡¨é€šé“æ•°ï¼ˆchannelsï¼‰ã€‚
  - `(patch_x x)` å’Œ `(patch_y y)` è¡¨ç¤ºå°†é«˜åº¦å’Œå®½åº¦åˆ†åˆ«åˆ†å‰²æˆ `patch_x` å’Œ `patch_y` å¤§å°çš„å—ï¼Œ`x` å’Œ `y` æ˜¯è¿™äº›å—çš„æ•°é‡ã€‚
- `-> b (x y) (patch_x patch_y c)`ï¼šè¿™æ˜¯è¾“å‡ºæ¨¡å¼å­—ç¬¦ä¸²ï¼Œè¡¨ç¤ºé‡æ–°æ’åˆ—åçš„å½¢çŠ¶ï¼š
  - `b` ä»ç„¶æ˜¯æ‰¹æ¬¡å¤§å°ã€‚
  - `(x y)` è¡¨ç¤ºå°†åˆ†å‰²åçš„å—é‡æ–°æ’åˆ—æˆä¸€ä¸ªæ–°çš„äºŒç»´å½¢çŠ¶ï¼Œ`x` å’Œ `y` åˆ†åˆ«æ˜¯å—åœ¨é«˜åº¦å’Œå®½åº¦ä¸Šçš„æ•°é‡ã€‚
  - `(patch_x patch_y c)` è¡¨ç¤ºæ¯ä¸ªå—çš„å½¢çŠ¶ï¼Œ`patch_x` å’Œ `patch_y` æ˜¯å—çš„ç©ºé—´ç»´åº¦ï¼Œ`c` æ˜¯é€šé“æ•°ã€‚
- `patch_x=self.patch_size` å’Œ `patch_y=self.patch_size`ï¼šè¿™äº›æ˜¯é¢å¤–çš„å‚æ•°ï¼Œç”¨äºæ›¿æ¢æ¨¡å¼å­—ç¬¦ä¸²ä¸­çš„ `patch_x` å’Œ `patch_y` å˜é‡ï¼Œå®ƒä»¬è¡¨ç¤ºæ¯ä¸ªå—çš„ç©ºé—´å°ºå¯¸ã€‚

æ€»çš„æ¥è¯´ï¼Œè¿™è¡Œä»£ç çš„ä½œç”¨æ˜¯å°†è¾“å…¥å›¾åƒåˆ†å‰²æˆ `patch_size x patch_size` çš„å°å—ï¼Œå¹¶å°†è¿™äº›å—é‡æ–°æ’åˆ—æˆä¸€ä¸ªä¸€ç»´çš„åºåˆ—ï¼Œæ¯ä¸ªå—åŒ…å« `patch_size * patch_size * channels` ä¸ªå…ƒç´ ï¼Œè¿™æ ·å°±å¯ä»¥å°†è¿™äº›å—ä½œä¸ºåºåˆ—è¾“å…¥åˆ°Transformeræ¨¡å‹ä¸­ã€‚





## è‡ªåŠ¨æ±‚å¯¼

```python
def f(a):
    b = a * 2
    while b.norm() < 1000:
        print("\n",b.norm())
        b = b * 2
    if b.sum() > 0:
        c = b
        print("C==b\n",c)
    else:
        c = 100 * b
        print("c=100b\n",c)
    return c

a = torch.randn(size=(3,1), requires_grad=True)
print(a.shape)
print(a)
d = f(a)
d.backward() #<====== run time error if a is vector or matrix RuntimeError: grad can be implicitly created only for scalar outputs
d.sum().backward() #<===== this way it will work
print(d)
```

å°è¯•å¯¹`d`è°ƒç”¨`.backward()`ï¼Œå¦‚æœ`a`æ˜¯å‘é‡æˆ–çŸ©é˜µï¼Œè¿™å°†å¼•å‘è¿è¡Œæ—¶é”™è¯¯ï¼š`RuntimeError: grad can be implicitly created only for scalar outputs`ã€‚ç¿»è¯‘ä¸ºï¼šè¿è¡Œæ—¶é”™è¯¯ï¼šæ¢¯åº¦åªèƒ½ä¸ºæ ‡é‡è¾“å‡ºéšå¼åˆ›å»ºã€‚é€šè¿‡è°ƒç”¨`d.sum().backward()`ï¼Œå¯ä»¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œå› ä¸ºè¿™æ ·ä¼šåˆ›å»ºä¸€ä¸ªæ ‡é‡è¾“å‡ºï¼Œå…è®¸æ¢¯åº¦è¢«éšå¼åˆ›å»ºã€‚



å› ä¸ºæ˜¯sumï¼Œæ‰€ä»¥å®é™…ä¸Šæ˜¯x1+x2+x3(size=(3,1))ï¼Œæ‰€ä»¥æ±‚å¾—çš„å¯¹xçš„æ¢¯åº¦

### ä¾‹å­

```python
%matplotlib inline
import matplotlib.pylab as plt
from matplotlib.ticker import FuncFormatter, MultipleLocator
import numpy as np
import torch

f,ax=plt.subplots(1)

x = np.linspace(-3np.pi, 3np.pi, 100)
x1= torch.tensor(x, requires_grad=True)
y1= torch.sin(x1)
y1.sum().backward()

ax.plot(x,np.sin(x),label=â€˜sin(x)â€™)
ax.plot(x,x1.grad,label=â€œgradient of sin(x)â€)
ax.legend(loc=â€˜upper centerâ€™, shadow=True)

ax.xaxis.set_major_formatter(FuncFormatter(
lambda val,pos: â€˜{:.0g}$\pi$â€™.format(val/np.pi) if val !=0 else â€˜0â€™
))
ax.xaxis.set_major_locator(MultipleLocator(base=np.pi))

plt.show()
```

x1.gradä¸ºcosæ˜¯å› ä¸ºy1æ˜¯sumçš„åå‘ä¼ æ’­ï¼Œç›¸å½“äºå¯¹æ¯ä¸ªx1é‡Œé¢çš„x1iç´¯åŠ çš„åˆ†é‡æ±‚å¯¼

## nn

### module

```python
import torch
from torch import nn
from torch.nn import functional as F

net = nn.Sequential(nn.Linear(20, 256), nn.ReLU(), nn.Linear(256, 10)) # æƒé‡åç½®é»˜è®¤éšæœº

X = torch.rand(2, 20) # 1ä¸ªå¤§å°ä¸º2*20çš„çŸ©é˜µä½œä¸ºè¾“å…¥
print(X)
net(X)
```

åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œæˆ‘ä»¬é€šè¿‡å®ä¾‹åŒ–`nn.Sequential`æ¥æ„å»ºæˆ‘ä»¬çš„æ¨¡å‹ï¼Œå±‚çš„æ‰§è¡Œé¡ºåºæ˜¯ä½œä¸ºå‚æ•°ä¼ é€’çš„ã€‚ç®€è€Œè¨€ä¹‹ï¼Œ(**`nn.Sequential`å®šä¹‰äº†ä¸€ç§ç‰¹æ®Šçš„`Module`**)ï¼Œå³åœ¨PyTorchä¸­è¡¨ç¤ºä¸€ä¸ªå—çš„ç±»ï¼Œå®ƒç»´æŠ¤äº†ä¸€ä¸ªç”±`Module`ç»„æˆçš„æœ‰åºåˆ—è¡¨ã€‚æ³¨æ„ï¼Œä¸¤ä¸ªå…¨è¿æ¥å±‚éƒ½æ˜¯`Linear`ç±»çš„å®ä¾‹ï¼Œ`Linear`ç±»æœ¬èº«å°±æ˜¯`Module`çš„å­ç±»ã€‚å¦å¤–ï¼Œåˆ°ç›®å‰ä¸ºæ­¢ï¼Œæˆ‘ä»¬ä¸€ç›´åœ¨é€šè¿‡`net(X)`è°ƒç”¨æˆ‘ä»¬çš„æ¨¡å‹æ¥è·å¾—æ¨¡å‹çš„è¾“å‡ºã€‚è¿™å®é™…ä¸Šæ˜¯`net.__call__(X)`çš„ç®€å†™ã€‚è¿™ä¸ªå‰å‘ä¼ æ’­å‡½æ•°éå¸¸ç®€å•ï¼šå®ƒå°†åˆ—è¡¨ä¸­çš„æ¯ä¸ªå—è¿æ¥åœ¨ä¸€èµ·ï¼Œå°†æ¯ä¸ªå—çš„è¾“å‡ºä½œä¸ºä¸‹ä¸€ä¸ªå—çš„è¾“å…¥ã€‚



```python
class MLP(nn.Module):
    # ç”¨æ¨¡å‹å‚æ•°å£°æ˜å±‚ã€‚è¿™é‡Œï¼Œæˆ‘ä»¬å£°æ˜ä¸¤ä¸ªå…¨è¿æ¥çš„å±‚
    def __init__(self):
        # è°ƒç”¨MLPçš„çˆ¶ç±»Moduleçš„æ„é€ å‡½æ•°æ¥æ‰§è¡Œå¿…è¦çš„åˆå§‹åŒ–ã€‚
        # è¿™æ ·ï¼Œåœ¨ç±»å®ä¾‹åŒ–æ—¶ä¹Ÿå¯ä»¥æŒ‡å®šå…¶ä»–å‡½æ•°å‚æ•°ï¼Œä¾‹å¦‚æ¨¡å‹å‚æ•°paramsï¼ˆç¨åå°†ä»‹ç»ï¼‰
        super().__init__()
        self.hidden = nn.Linear(20, 256)  # éšè—å±‚
        self.out = nn.Linear(256, 10)  # è¾“å‡ºå±‚

    # å®šä¹‰æ¨¡å‹çš„å‰å‘ä¼ æ’­ï¼Œå³å¦‚ä½•æ ¹æ®è¾“å…¥Xè¿”å›æ‰€éœ€çš„æ¨¡å‹è¾“å‡º
    def forward(self, X):
        # æ³¨æ„ï¼Œè¿™é‡Œæˆ‘ä»¬ä½¿ç”¨ReLUçš„å‡½æ•°ç‰ˆæœ¬ï¼Œå…¶åœ¨nn.functionalæ¨¡å—ä¸­å®šä¹‰ã€‚
        return self.out(F.relu(self.hidden(X)))
```

æˆ‘ä»¬é¦–å…ˆçœ‹ä¸€ä¸‹å‰å‘ä¼ æ’­å‡½æ•°ï¼Œå®ƒä»¥`X`ä½œä¸ºè¾“å…¥ï¼Œ è®¡ç®—å¸¦æœ‰æ¿€æ´»å‡½æ•°çš„éšè—è¡¨ç¤ºï¼Œå¹¶è¾“å‡ºå…¶æœªè§„èŒƒåŒ–çš„è¾“å‡ºå€¼ã€‚ åœ¨è¿™ä¸ª`MLP`å®ç°ä¸­ï¼Œä¸¤ä¸ªå±‚éƒ½æ˜¯å®ä¾‹å˜é‡ã€‚ è¦äº†è§£è¿™ä¸ºä»€ä¹ˆæ˜¯åˆç†çš„ï¼Œå¯ä»¥æƒ³è±¡å®ä¾‹åŒ–ä¸¤ä¸ªå¤šå±‚æ„ŸçŸ¥æœºï¼ˆ`net1`å’Œ`net2`ï¼‰ï¼Œ å¹¶æ ¹æ®ä¸åŒçš„æ•°æ®å¯¹å®ƒä»¬è¿›è¡Œè®­ç»ƒã€‚ å½“ç„¶ï¼Œæˆ‘ä»¬å¸Œæœ›å®ƒä»¬å­¦åˆ°ä¸¤ç§ä¸åŒçš„æ¨¡å‹ã€‚

æ¥ç€æˆ‘ä»¬[**å®ä¾‹åŒ–å¤šå±‚æ„ŸçŸ¥æœºçš„å±‚ï¼Œç„¶ååœ¨æ¯æ¬¡è°ƒç”¨å‰å‘ä¼ æ’­å‡½æ•°æ—¶è°ƒç”¨è¿™äº›å±‚**]ã€‚ æ³¨æ„ä¸€äº›å…³é”®ç»†èŠ‚ï¼š é¦–å…ˆï¼Œæˆ‘ä»¬å®šåˆ¶çš„`__init__`å‡½æ•°é€šè¿‡`super().__init__()` è°ƒç”¨çˆ¶ç±»çš„`__init__`å‡½æ•°ï¼Œ çœå»äº†é‡å¤ç¼–å†™æ¨¡ç‰ˆä»£ç çš„ç—›è‹¦ã€‚ ç„¶åï¼Œæˆ‘ä»¬å®ä¾‹åŒ–ä¸¤ä¸ªå…¨è¿æ¥å±‚ï¼Œ åˆ†åˆ«ä¸º`self.hidden`å’Œ`self.out`ã€‚ æ³¨æ„ï¼Œé™¤éæˆ‘ä»¬å®ç°ä¸€ä¸ªæ–°çš„è¿ç®—ç¬¦ï¼Œ å¦åˆ™æˆ‘ä»¬ä¸å¿…æ‹…å¿ƒåå‘ä¼ æ’­å‡½æ•°æˆ–å‚æ•°åˆå§‹åŒ–ï¼Œ ç³»ç»Ÿå°†è‡ªåŠ¨ç”Ÿæˆè¿™äº›ã€‚



### Sequential

ç°åœ¨æˆ‘ä»¬å¯ä»¥æ›´ä»”ç»†åœ°çœ‹çœ‹`Sequential`ç±»æ˜¯å¦‚ä½•å·¥ä½œçš„ï¼Œ å›æƒ³ä¸€ä¸‹`Sequential`çš„è®¾è®¡æ˜¯ä¸ºäº†æŠŠå…¶ä»–æ¨¡å—ä¸²èµ·æ¥ã€‚ ä¸ºäº†æ„å»ºæˆ‘ä»¬è‡ªå·±çš„ç®€åŒ–çš„`MySequential`ï¼Œ æˆ‘ä»¬åªéœ€è¦å®šä¹‰ä¸¤ä¸ªå…³é”®å‡½æ•°ï¼š

1. ä¸€ç§å°†å—é€ä¸ªè¿½åŠ åˆ°åˆ—è¡¨ä¸­çš„å‡½æ•°ï¼›
2. ä¸€ç§å‰å‘ä¼ æ’­å‡½æ•°ï¼Œç”¨äºå°†è¾“å…¥æŒ‰è¿½åŠ å—çš„é¡ºåºä¼ é€’ç»™å—ç»„æˆçš„â€œé“¾æ¡â€ã€‚

ä¸‹é¢çš„`MySequential`ç±»æä¾›äº†ä¸é»˜è®¤`Sequential`ç±»ç›¸åŒçš„åŠŸèƒ½ã€‚

```python
class MySequential(nn.Module):
    def __init__(self, *args):
        super().__init__()
        for idx, module in enumerate(args):
            # è¿™é‡Œï¼Œmoduleæ˜¯Moduleå­ç±»çš„ä¸€ä¸ªå®ä¾‹ã€‚æˆ‘ä»¬æŠŠå®ƒä¿å­˜åœ¨'Module'ç±»çš„æˆå‘˜
            # å˜é‡_modulesä¸­ã€‚_moduleçš„ç±»å‹æ˜¯OrderedDict
            self._modules[str(idx)] = module

    def forward(self, X):
        # OrderedDictä¿è¯äº†æŒ‰ç…§æˆå‘˜æ·»åŠ çš„é¡ºåºéå†å®ƒä»¬
        for block in self._modules.values():
            X = block(X)
        return X
```

`__init__`å‡½æ•°å°†æ¯ä¸ªæ¨¡å—é€ä¸ªæ·»åŠ åˆ°æœ‰åºå­—å…¸`_modules`ä¸­ã€‚ è¯»è€…å¯èƒ½ä¼šå¥½å¥‡ä¸ºä»€ä¹ˆæ¯ä¸ª`Module`éƒ½æœ‰ä¸€ä¸ª`_modules`å±æ€§ï¼Ÿ ä»¥åŠä¸ºä»€ä¹ˆæˆ‘ä»¬ä½¿ç”¨å®ƒè€Œä¸æ˜¯è‡ªå·±å®šä¹‰ä¸€ä¸ªPythonåˆ—è¡¨ï¼Ÿ ç®€è€Œè¨€ä¹‹ï¼Œ`_modules`çš„ä¸»è¦ä¼˜ç‚¹æ˜¯ï¼š åœ¨æ¨¡å—çš„å‚æ•°åˆå§‹åŒ–è¿‡ç¨‹ä¸­ï¼Œ ç³»ç»ŸçŸ¥é“åœ¨`_modules`å­—å…¸ä¸­æŸ¥æ‰¾éœ€è¦åˆå§‹åŒ–å‚æ•°çš„å­å—ã€‚

å½“`MySequential`çš„å‰å‘ä¼ æ’­å‡½æ•°è¢«è°ƒç”¨æ—¶ï¼Œ æ¯ä¸ªæ·»åŠ çš„å—éƒ½æŒ‰ç…§å®ƒä»¬è¢«æ·»åŠ çš„é¡ºåºæ‰§è¡Œã€‚ ç°åœ¨å¯ä»¥ä½¿ç”¨æˆ‘ä»¬çš„`MySequential`ç±»é‡æ–°å®ç°å¤šå±‚æ„ŸçŸ¥æœºã€‚

```python
net = MySequential(nn.Linear(20, 256), nn.ReLU(), nn.Linear(256, 10))
net(X)
```

### forward

`Sequential`ç±»ä½¿æ¨¡å‹æ„é€ å˜å¾—ç®€å•ï¼Œ å…è®¸æˆ‘ä»¬ç»„åˆæ–°çš„æ¶æ„ï¼Œè€Œä¸å¿…å®šä¹‰è‡ªå·±çš„ç±»ã€‚ ç„¶è€Œï¼Œå¹¶ä¸æ˜¯æ‰€æœ‰çš„æ¶æ„éƒ½æ˜¯ç®€å•çš„é¡ºåºæ¶æ„ã€‚ å½“éœ€è¦æ›´å¼ºçš„çµæ´»æ€§æ—¶ï¼Œæˆ‘ä»¬éœ€è¦å®šä¹‰è‡ªå·±çš„å—ã€‚ ä¾‹å¦‚ï¼Œæˆ‘ä»¬å¯èƒ½å¸Œæœ›åœ¨å‰å‘ä¼ æ’­å‡½æ•°ä¸­æ‰§è¡ŒPythonçš„æ§åˆ¶æµã€‚ æ­¤å¤–ï¼Œæˆ‘ä»¬å¯èƒ½å¸Œæœ›æ‰§è¡Œä»»æ„çš„æ•°å­¦è¿ç®—ï¼Œ è€Œä¸æ˜¯ç®€å•åœ°ä¾èµ–é¢„å®šä¹‰çš„ç¥ç»ç½‘ç»œå±‚ã€‚

åˆ°ç›®å‰ä¸ºæ­¢ï¼Œ æˆ‘ä»¬ç½‘ç»œä¸­çš„æ‰€æœ‰æ“ä½œéƒ½å¯¹ç½‘ç»œçš„æ¿€æ´»å€¼åŠç½‘ç»œçš„å‚æ•°èµ·ä½œç”¨ã€‚ ç„¶è€Œï¼Œæœ‰æ—¶æˆ‘ä»¬å¯èƒ½å¸Œæœ›åˆå¹¶æ—¢ä¸æ˜¯ä¸Šä¸€å±‚çš„ç»“æœä¹Ÿä¸æ˜¯å¯æ›´æ–°å‚æ•°çš„é¡¹ï¼Œ æˆ‘ä»¬ç§°ä¹‹ä¸º*å¸¸æ•°å‚æ•°*ï¼ˆconstant parameterï¼‰ã€‚ ä¾‹å¦‚ï¼Œæˆ‘ä»¬éœ€è¦ä¸€ä¸ªè®¡ç®—å‡½æ•° ğ‘“(ğ‘¥,ğ‘¤)=ğ‘â‹…ğ‘¤âŠ¤ğ‘¥çš„å±‚ï¼Œ å…¶ä¸­ğ‘¥æ˜¯è¾“å…¥ï¼Œ ğ‘¤æ˜¯å‚æ•°ï¼Œ ğ‘æ˜¯æŸä¸ªåœ¨ä¼˜åŒ–è¿‡ç¨‹ä¸­æ²¡æœ‰æ›´æ–°çš„æŒ‡å®šå¸¸é‡ã€‚ å› æ­¤æˆ‘ä»¬å®ç°äº†ä¸€ä¸ª`FixedHiddenMLP`ç±»ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š

```python
class FixedHiddenMLP(nn.Module):
    def __init__(self):
        super().__init__()
        # ä¸è®¡ç®—æ¢¯åº¦çš„éšæœºæƒé‡å‚æ•°ã€‚å› æ­¤å…¶åœ¨è®­ç»ƒæœŸé—´ä¿æŒä¸å˜
        self.rand_weight = torch.rand((20, 20), requires_grad=False)
        self.linear = nn.Linear(20, 20)

    def forward(self, X):
        X = self.linear(X)
        # ä½¿ç”¨åˆ›å»ºçš„å¸¸é‡å‚æ•°ä»¥åŠreluå’Œmmå‡½æ•°
        X = F.relu(torch.mm(X, self.rand_weight) + 1)
        # å¤ç”¨å…¨è¿æ¥å±‚ã€‚è¿™ç›¸å½“äºä¸¤ä¸ªå…¨è¿æ¥å±‚å…±äº«å‚æ•°
        X = self.linear(X)
        # æ§åˆ¶æµ
        while X.abs().sum() > 1:
            X /= 2
        return X.sum()
```

åœ¨è¿™ä¸ª`FixedHiddenMLP`æ¨¡å‹ä¸­ï¼Œæˆ‘ä»¬å®ç°äº†ä¸€ä¸ªéšè—å±‚ï¼Œ å…¶æƒé‡ï¼ˆ`self.rand_weight`ï¼‰åœ¨å®ä¾‹åŒ–æ—¶è¢«éšæœºåˆå§‹åŒ–ï¼Œä¹‹åä¸ºå¸¸é‡ã€‚ è¿™ä¸ªæƒé‡ä¸æ˜¯ä¸€ä¸ªæ¨¡å‹å‚æ•°ï¼Œå› æ­¤å®ƒæ°¸è¿œä¸ä¼šè¢«åå‘ä¼ æ’­æ›´æ–°ã€‚ ç„¶åï¼Œç¥ç»ç½‘ç»œå°†è¿™ä¸ªå›ºå®šå±‚çš„è¾“å‡ºé€šè¿‡ä¸€ä¸ªå…¨è¿æ¥å±‚ã€‚

æ³¨æ„ï¼Œåœ¨è¿”å›è¾“å‡ºä¹‹å‰ï¼Œæ¨¡å‹åšäº†ä¸€äº›ä¸å¯»å¸¸çš„äº‹æƒ…ï¼š å®ƒè¿è¡Œäº†ä¸€ä¸ªwhileå¾ªç¯ï¼Œåœ¨ğ¿1èŒƒæ•°å¤§äº1çš„æ¡ä»¶ä¸‹ï¼Œ å°†è¾“å‡ºå‘é‡é™¤ä»¥2ï¼Œç›´åˆ°å®ƒæ»¡è¶³æ¡ä»¶ä¸ºæ­¢ã€‚ æœ€åï¼Œæ¨¡å‹è¿”å›äº†`X`ä¸­æ‰€æœ‰é¡¹çš„å’Œã€‚ æ³¨æ„ï¼Œæ­¤æ“ä½œå¯èƒ½ä¸ä¼šå¸¸ç”¨äºåœ¨ä»»ä½•å®é™…ä»»åŠ¡ä¸­ï¼Œ æˆ‘ä»¬åªå±•ç¤ºå¦‚ä½•å°†ä»»æ„ä»£ç é›†æˆåˆ°ç¥ç»ç½‘ç»œè®¡ç®—çš„æµç¨‹ä¸­ã€‚

### æ··åˆ

æˆ‘ä»¬å¯ä»¥[**æ··åˆæ­é…å„ç§ç»„åˆå—çš„æ–¹æ³•**]ã€‚ åœ¨ä¸‹é¢çš„ä¾‹å­ä¸­ï¼Œæˆ‘ä»¬ä»¥ä¸€äº›æƒ³åˆ°çš„æ–¹æ³•åµŒå¥—å—ã€‚



```python
class NestMLP(nn.Module):
    def __init__(self):
        super().__init__()
        self.net = nn.Sequential(nn.Linear(20, 64), nn.ReLU(),
                                 nn.Linear(64, 32), nn.ReLU())
        self.linear = nn.Linear(32, 16)

    def forward(self, X):
        return self.linear(self.net(X))

chimera = nn.Sequential(NestMLP(), nn.Linear(16, 20), FixedHiddenMLP())
chimera(X)
```

### æ·»åŠ å—

```python
def block1():
    return nn.Sequential(nn.Linear(4, 8), nn.ReLU(),
                         nn.Linear(8, 4), nn.ReLU())

def block2():
    net = nn.Sequential()
    for i in range(4):
        # åœ¨è¿™é‡ŒåµŒå¥—
        net.add_module(f'block {i}', block1())
    return net

rgnet = nn.Sequential(block2(), nn.Linear(4, 1))
rgnet(X)
```

printå¾—åˆ°ï¼š

```
Sequential(
  (0): Sequential(
    (block 0): Sequential(
      (0): Linear(in_features=4, out_features=8, bias=True)
      (1): ReLU()
      (2): Linear(in_features=8, out_features=4, bias=True)
      (3): ReLU()
    )
    (block 1): Sequential(
      (0): Linear(in_features=4, out_features=8, bias=True)
      (1): ReLU()
      (2): Linear(in_features=8, out_features=4, bias=True)
      (3): ReLU()
    )
    (block 2): Sequential(
      (0): Linear(in_features=4, out_features=8, bias=True)
      (1): ReLU()
      (2): Linear(in_features=8, out_features=4, bias=True)
      (3): ReLU()
    )
    (block 3): Sequential(
      (0): Linear(in_features=4, out_features=8, bias=True)
      (1): ReLU()
      (2): Linear(in_features=8, out_features=4, bias=True)
      (3): ReLU()
    )
  )
  (1): Linear(in_features=4, out_features=1, bias=True)
)
```







## å‚æ•°ç®¡ç†

### è®¿é—®å‚æ•°

```python
import torch
from torch import nn

net = nn.Sequential(nn.Linear(4, 8), nn.ReLU(), nn.Linear(8, 1))
X = torch.rand(size=(2, 4))
net(X)
```

æˆ‘ä»¬ä»å·²æœ‰æ¨¡å‹ä¸­è®¿é—®å‚æ•°ã€‚ å½“é€šè¿‡`Sequential`ç±»å®šä¹‰æ¨¡å‹æ—¶ï¼Œ æˆ‘ä»¬å¯ä»¥é€šè¿‡ç´¢å¼•æ¥è®¿é—®æ¨¡å‹çš„ä»»æ„å±‚ã€‚ è¿™å°±åƒæ¨¡å‹æ˜¯ä¸€ä¸ªåˆ—è¡¨ä¸€æ ·ï¼Œæ¯å±‚çš„å‚æ•°éƒ½åœ¨å…¶å±æ€§ä¸­ã€‚ å¦‚ä¸‹æ‰€ç¤ºï¼Œæˆ‘ä»¬å¯ä»¥æ£€æŸ¥ç¬¬äºŒä¸ªå…¨è¿æ¥å±‚çš„å‚æ•°ã€‚

```python
print(net[2].state_dict())
```

```
OrderedDict([('weight', tensor([[ 0.2184,  0.1997, -0.2909,  0.1927, -0.0060,  0.3408, -0.2632,  0.1543]])), ('bias', tensor([0.3183]))])
```

è¾“å‡ºçš„ç»“æœå‘Šè¯‰æˆ‘ä»¬ä¸€äº›é‡è¦çš„äº‹æƒ…ï¼š é¦–å…ˆï¼Œè¿™ä¸ªå…¨è¿æ¥å±‚åŒ…å«ä¸¤ä¸ªå‚æ•°ï¼Œåˆ†åˆ«æ˜¯è¯¥å±‚çš„æƒé‡å’Œåç½®ã€‚ ä¸¤è€…éƒ½å­˜å‚¨ä¸ºå•ç²¾åº¦æµ®ç‚¹æ•°ï¼ˆfloat32ï¼‰ã€‚ æ³¨æ„ï¼Œå‚æ•°åç§°å…è®¸å”¯ä¸€æ ‡è¯†æ¯ä¸ªå‚æ•°ï¼Œå³ä½¿åœ¨åŒ…å«æ•°ç™¾ä¸ªå±‚çš„ç½‘ç»œä¸­ä¹Ÿæ˜¯å¦‚æ­¤ã€‚



æ³¨æ„ï¼Œæ¯ä¸ªå‚æ•°éƒ½è¡¨ç¤ºä¸ºå‚æ•°ç±»çš„ä¸€ä¸ªå®ä¾‹ã€‚ è¦å¯¹å‚æ•°æ‰§è¡Œä»»ä½•æ“ä½œï¼Œé¦–å…ˆæˆ‘ä»¬éœ€è¦è®¿é—®åº•å±‚çš„æ•°å€¼ã€‚ æœ‰å‡ ç§æ–¹æ³•å¯ä»¥åšåˆ°è¿™ä¸€ç‚¹ã€‚æœ‰äº›æ¯”è¾ƒç®€å•ï¼Œè€Œå¦ä¸€äº›åˆ™æ¯”è¾ƒé€šç”¨ã€‚ ä¸‹é¢çš„ä»£ç ä»ç¬¬äºŒä¸ªå…¨è¿æ¥å±‚ï¼ˆå³ç¬¬ä¸‰ä¸ªç¥ç»ç½‘ç»œå±‚ï¼‰æå–åç½®ï¼Œ æå–åè¿”å›çš„æ˜¯ä¸€ä¸ªå‚æ•°ç±»å®ä¾‹ï¼Œå¹¶è¿›ä¸€æ­¥è®¿é—®è¯¥å‚æ•°çš„å€¼ã€‚

```python
print(type(net[2].bias))
print(net[2].bias)
print(net[2].bias.data)
```

```
<class 'torch.nn.parameter.Parameter'>
Parameter containing:
tensor([0.3183], requires_grad=True)
tensor([0.3183])
```



### åˆå§‹åŒ–å‚æ•°

è®©æˆ‘ä»¬é¦–å…ˆè°ƒç”¨å†…ç½®çš„åˆå§‹åŒ–å™¨ã€‚ ä¸‹é¢çš„ä»£ç å°†æ‰€æœ‰æƒé‡å‚æ•°åˆå§‹åŒ–ä¸ºæ ‡å‡†å·®ä¸º0.01çš„é«˜æ–¯éšæœºå˜é‡ï¼Œ ä¸”å°†åç½®å‚æ•°è®¾ç½®ä¸º0ã€‚

```python
def init_normal(m):
    if type(m) == nn.Linear:
        nn.init.normal_(m.weight, mean=0, std=0.01)
        nn.init.zeros_(m.bias)
net.apply(init_normal)
net[0].weight.data[0], net[0].bias.data[0]
```

æˆ‘ä»¬è¿˜å¯ä»¥[**å¯¹æŸäº›å—åº”ç”¨ä¸åŒçš„åˆå§‹åŒ–æ–¹æ³•**]ã€‚ ä¾‹å¦‚ï¼Œä¸‹é¢æˆ‘ä»¬ä½¿ç”¨Xavieråˆå§‹åŒ–æ–¹æ³•åˆå§‹åŒ–ç¬¬ä¸€ä¸ªç¥ç»ç½‘ç»œå±‚ï¼Œ ç„¶åå°†ç¬¬ä¸‰ä¸ªç¥ç»ç½‘ç»œå±‚åˆå§‹åŒ–ä¸ºå¸¸é‡å€¼42ã€‚

```python
def init_xavier(m):
    if type(m) == nn.Linear:
        nn.init.xavier_uniform_(m.weight)
def init_42(m):
    if type(m) == nn.Linear:
        nn.init.constant_(m.weight, 42)

net[0].apply(init_xavier)
net[2].apply(init_42)
print(net[0].weight.data[0])
print(net[2].weight.data)
```

### è‡ªå®šä¹‰å±‚

æˆ‘ä»¬å¯ä»¥ä½¿ç”¨å†…ç½®å‡½æ•°æ¥åˆ›å»ºå‚æ•°ï¼Œè¿™äº›å‡½æ•°æä¾›ä¸€äº›åŸºæœ¬çš„ç®¡ç†åŠŸèƒ½ã€‚ æ¯”å¦‚ç®¡ç†è®¿é—®ã€åˆå§‹åŒ–ã€å…±äº«ã€ä¿å­˜å’ŒåŠ è½½æ¨¡å‹å‚æ•°ã€‚ è¿™æ ·åšçš„å¥½å¤„ä¹‹ä¸€æ˜¯ï¼šæˆ‘ä»¬ä¸éœ€è¦ä¸ºæ¯ä¸ªè‡ªå®šä¹‰å±‚ç¼–å†™è‡ªå®šä¹‰çš„åºåˆ—åŒ–ç¨‹åºã€‚

ç°åœ¨ï¼Œè®©æˆ‘ä»¬å®ç°è‡ªå®šä¹‰ç‰ˆæœ¬çš„å…¨è¿æ¥å±‚ã€‚ å›æƒ³ä¸€ä¸‹ï¼Œè¯¥å±‚éœ€è¦ä¸¤ä¸ªå‚æ•°ï¼Œä¸€ä¸ªç”¨äºè¡¨ç¤ºæƒé‡ï¼Œå¦ä¸€ä¸ªç”¨äºè¡¨ç¤ºåç½®é¡¹ã€‚ åœ¨æ­¤å®ç°ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨ä¿®æ­£çº¿æ€§å•å…ƒä½œä¸ºæ¿€æ´»å‡½æ•°ã€‚ è¯¥å±‚éœ€è¦è¾“å…¥å‚æ•°ï¼š`in_units`å’Œ`units`ï¼Œåˆ†åˆ«è¡¨ç¤ºè¾“å…¥æ•°å’Œè¾“å‡ºæ•°ã€‚

```python
class MyLinear(nn.Module):
    def __init__(self, in_units, units):
        super().__init__()
        self.weight = nn.Parameter(torch.randn(in_units, units))
        self.bias = nn.Parameter(torch.randn(units,))
    def forward(self, X):
        linear = torch.matmul(X, self.weight.data) + self.bias.data
        return F.relu(linear)
```

ç”¨nn.Parameterä½¿å¾—wå’Œbä¿å­˜æ¢¯åº¦

## åŠ è½½ã€ä¿å­˜ä¸­é—´ç»“æœ

### tensor

```python
import torch
from torch import nn
from torch.nn import functional as F

x1 = torch.arange(4)
torch.save(x1, 'x-file') # ä¿å­˜ï¼ŒåŒç›®å½•ä¸‹å­˜ä¸€ä¸ªæ–‡ä»¶
x2 = torch.load('x-file') # åŠ è½½ï¼ŒåŒç›®å½•ä¸‹åŠ è½½x-fileæ–‡ä»¶
```

### dict

```python
mydict = {'x': x, 'y': y}
torch.save(mydict, 'mydict')
mydict2 = torch.load('mydict')
```

### åŠ è½½ã€ä¿å­˜æ¨¡å‹



```python
class MLP(nn.Module):
    def __init__(self):
        super().__init__()
        self.hidden = nn.Linear(20, 256)
        self.output = nn.Linear(256, 10)

    def forward(self, x):
        return self.output(F.relu(self.hidden(x)))

net = MLP()
X = torch.randn(size=(2, 20))
Y = net(X)
```

æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬[**å°†æ¨¡å‹çš„å‚æ•°å­˜å‚¨åœ¨ä¸€ä¸ªå«åšâ€œmlp.paramsâ€çš„æ–‡ä»¶ä¸­ã€‚**]

```python
torch.save(net.state_dict(), 'mlp.params')
```

ä¸ºäº†æ¢å¤æ¨¡å‹ï¼Œæˆ‘ä»¬[**å®ä¾‹åŒ–äº†åŸå§‹å¤šå±‚æ„ŸçŸ¥æœºæ¨¡å‹çš„ä¸€ä¸ªå¤‡ä»½ã€‚**] è¿™é‡Œæˆ‘ä»¬ä¸éœ€è¦éšæœºåˆå§‹åŒ–æ¨¡å‹å‚æ•°ï¼Œè€Œæ˜¯(**ç›´æ¥è¯»å–æ–‡ä»¶ä¸­å­˜å‚¨çš„å‚æ•°ã€‚**)

```python
clone = MLP()
clone.load_state_dict(torch.load('mlp.params'))
clone.eval()
```

## expand

æ‰©å±•wei'du

```python
# æ‰©å±•cls_tokenä»¥åŒ¹é…batch_sizeï¼Œå¹¶ä¿æŒå…¶ä»–ç»´åº¦ä¸å˜
# expandéœ€è¦æŒ‡å®šæ‰©å±•çš„ç»´åº¦ï¼Œæ‰€ä»¥è¿™é‡Œæ¯ä¸ªdiméƒ½éœ€è¦ç”¨-1æ¥è¡¨ç¤º
cls_token = self.cls_token.expand(x.shape[0], -1, -1)
```

## ä½¿ç”¨GPU

### æŸ¥çœ‹å‚æ•°æ‰€åœ¨è®¾å¤‡

```python
X1 = torch.rand(3, 64)
X2 = torch.rand(3, 64)
X1.device
```

```
device(type='cpu')
```



###  å°†æ¨¡å‹æ”¾åœ¨GPUä¸Š

ææ²é¢„è®¾äº†ä¸€ä¸ªGPUçš„å‡½æ•°

```python
def try_gpu(i=0):  #@save
    """å¦‚æœå­˜åœ¨ï¼Œåˆ™è¿”å›gpu(i)ï¼Œå¦åˆ™è¿”å›cpu()"""
    if torch.cuda.device_count() >= i + 1:
        return torch.device(f'cuda:{i}')
    return torch.device('cpu')

def try_all_gpus():  #@save
    """è¿”å›æ‰€æœ‰å¯ç”¨çš„GPUï¼Œå¦‚æœæ²¡æœ‰GPUï¼Œåˆ™è¿”å›[cpu(),]"""
    devices = [torch.device(f'cuda:{i}')
             for i in range(torch.cuda.device_count())]
    return devices if devices else [torch.device('cpu')]
```

å°†æ¨¡å‹æ”¾åœ¨GPUä¸Š

```python
net = nn.Sequential(nn.Linear(3, 1))
net = net.to(device=try_gpu())
```

æˆ‘çš„ç”µè„‘æœ¬åœ°åªæœ‰ä¸€ä¸ªGPUï¼Œç­‰ä»·äºï¼š

```python
net = nn.Sequential(nn.Linear(3, 1))
net = net.to(device='cuda:0')
```

CPUã€GPUä½¿ç”¨åŒºåˆ«


```python
import time
import torch

def try_gpu(i=0):  #@save
    """å¦‚æœå­˜åœ¨ï¼Œåˆ™è¿”å›gpu(i)ï¼Œå¦åˆ™è¿”å›cpu()"""
    if torch.cuda.device_count() >= i + 1:
        return torch.device(f'cuda:{i}')
    return torch.device('cpu')

startTime1=time.time()
for i in range(100):
    A = torch.ones(500,500)
    B = torch.ones(500,500)
    C = torch.matmul(A,B)
endTime1=time.time()

startTime2=time.time()
for i in range(100):
    A = torch.ones(500,500,device=try_gpu())
    B = torch.ones(500,500,device=try_gpu())
    C = torch.matmul(A,B)
endTime2=time.time()

print('cpuè®¡ç®—æ€»æ—¶é•¿:', round((endTime1 - startTime1)*1000, 2),'ms')
print('gpuè®¡ç®—æ€»æ—¶é•¿:', round((endTime2 - startTime2)*1000, 2),'ms')
```





## çŸ©é˜µä¹˜æ³•

A.shape =ï¼ˆb,m,n)ï¼›B.shape = (b,n,k)
numpy.matmul(A,B) ç»“æœshapeä¸º(b,m,k)

ä¸¤ä¸ªTensorç»´åº¦è¦æ±‚ï¼š

- "2ç»´ä»¥ä¸Š"çš„å°ºå¯¸å¿…é¡»å®Œå…¨å¯¹åº”ç›¸ç­‰ï¼›
- "2ç»´"å…·æœ‰å®é™…æ„ä¹‰çš„å•ä½ï¼Œåªè¦æ»¡è¶³çŸ©é˜µç›¸ä¹˜çš„å°ºå¯¸è§„å¾‹å³å¯ã€‚



ç¬¬ä¸€ä¸ªå‚æ•°çš„æœ€åä¸€ä¸ªç»´åº¦å’Œç¬¬äºŒä¸ªå‚æ•°çš„å€’æ•°ç¬¬äºŒä¸ªç»´åº¦çš„å°ºå¯¸ï¼ˆå³ç»´åº¦å¤§å°ï¼‰å¿…é¡»ç›¸åŒ







## å¹¿æ’­æ±‚å’Œ

```python
#@save
class AdditiveAttention(nn.Module):
    """åŠ æ€§æ³¨æ„åŠ›"""
    def __init__(self, key_size, query_size, num_hiddens, dropout, **kwargs):
        super(AdditiveAttention, self).__init__(**kwargs)
        self.W_k = nn.Linear(key_size, num_hiddens, bias=False)
        self.W_q = nn.Linear(query_size, num_hiddens, bias=False)
        self.w_v = nn.Linear(num_hiddens, 1, bias=False)
        self.dropout = nn.Dropout(dropout)

    def forward(self, queries, keys, values, valid_lens):
        queries, keys = self.W_q(queries), self.W_k(keys)
        # åœ¨ç»´åº¦æ‰©å±•åï¼Œ
        # queriesçš„å½¢çŠ¶ï¼š(batch_sizeï¼ŒæŸ¥è¯¢çš„ä¸ªæ•°ï¼Œ1ï¼Œnum_hidden)
        # keyçš„å½¢çŠ¶ï¼š(batch_sizeï¼Œ1ï¼Œâ€œé”®ï¼å€¼â€å¯¹çš„ä¸ªæ•°ï¼Œnum_hiddens)
        # ä½¿ç”¨å¹¿æ’­æ–¹å¼è¿›è¡Œæ±‚å’Œ
        features = queries.unsqueeze(2) + keys.unsqueeze(1)
        features = torch.tanh(features)
        # self.w_vä»…æœ‰ä¸€ä¸ªè¾“å‡ºï¼Œå› æ­¤ä»å½¢çŠ¶ä¸­ç§»é™¤æœ€åé‚£ä¸ªç»´åº¦ã€‚
        # scoresçš„å½¢çŠ¶ï¼š(batch_sizeï¼ŒæŸ¥è¯¢çš„ä¸ªæ•°ï¼Œâ€œé”®-å€¼â€å¯¹çš„ä¸ªæ•°)
        scores = self.w_v(features).squeeze(-1)
        self.attention_weights = masked_softmax(scores, valid_lens)
        # valuesçš„å½¢çŠ¶ï¼š(batch_sizeï¼Œâ€œé”®ï¼å€¼â€å¯¹çš„ä¸ªæ•°ï¼Œå€¼çš„ç»´åº¦)
        return torch.bmm(self.dropout(self.attention_weights), values)
```



å¦‚å¯¹ä¸€ä¸ª1\*3çš„å‘é‡å’Œä¸€ä¸ª3\*1çš„å‘é‡æ±‚å’Œï¼Œå¦‚1\*3çš„æ˜¯[1, 2, 3]ï¼Œ3\*1çš„æ˜¯[4, 5, 6]^Tã€‚é‚£ä¹ˆå˜æˆï¼Œç„¶åç›¸åŠ 

```
[[1, 2 ,3], 
 [1, 2 ,3], 
 [1, 2 ,3]]
 
 [[4, 4 ,4], 
  [5, 5 ,5], 
  [6, 6 ,6]]
```

## Datasetç±»

`Dataset`ç±»æ˜¯PyTorchä¸­çš„ä¸€ä¸ªæ ¸å¿ƒç»„ä»¶ï¼Œç”¨äºå®šä¹‰æ•°æ®é›†çš„ç»“æ„å’Œè¡Œä¸ºã€‚å®ƒæ˜¯ä¸€ä¸ªæŠ½è±¡åŸºç±»ï¼Œæä¾›äº†æ•°æ®åŠ è½½å’Œé¢„å¤„ç†çš„åŸºæœ¬æ¡†æ¶ï¼Œä½¿å¾—ç”¨æˆ·å¯ä»¥è‡ªå®šä¹‰æ•°æ®é›†çš„å¤„ç†æ–¹å¼ã€‚ä»¥ä¸‹æ˜¯`Dataset`ç±»çš„ä¸€äº›å…³é”®ç‰¹æ€§å’Œä½¿ç”¨æ–¹æ³•ï¼š

### åŸºæœ¬ç»“æ„

`Dataset`ç±»å®šä¹‰äº†ä¸‰ä¸ªä¸»è¦çš„æ–¹æ³•ï¼Œæ‰€æœ‰å­ç±»éƒ½éœ€è¦æ ¹æ®å…·ä½“æ•°æ®é›†å®ç°è¿™äº›æ–¹æ³•ï¼š

1. **`__init__`æ–¹æ³•**ï¼š
   - æ„é€ å‡½æ•°ï¼Œç”¨äºåˆå§‹åŒ–æ•°æ®é›†å¯¹è±¡ã€‚é€šå¸¸åœ¨è¿™é‡Œè®¾ç½®æ•°æ®è·¯å¾„ã€åŠ è½½æ•°æ®ã€é¢„å¤„ç†å‚æ•°ç­‰ã€‚

2. **`__getitem__`æ–¹æ³•**ï¼š
   - ç´¢å¼•æ–¹æ³•ï¼Œæ¥å—ä¸€ä¸ªç´¢å¼•å€¼`idx`ï¼Œå¹¶è¿”å›è¯¥ç´¢å¼•å¯¹åº”çš„æ•°æ®é¡¹ï¼ˆé€šå¸¸æ˜¯ä¸€ä¸ªæ ·æœ¬åŠå…¶æ ‡ç­¾ï¼‰ã€‚
   - è¿™ä¸ªæ–¹æ³•æ˜¯æ•°æ®åŠ è½½çš„æ ¸å¿ƒï¼Œå®ƒå®šä¹‰äº†å¦‚ä½•æ ¹æ®ç´¢å¼•åŠ è½½å’Œå¤„ç†å•ä¸ªæ•°æ®æ ·æœ¬ã€‚

3. **`__len__`æ–¹æ³•**ï¼š
   - è¿”å›æ•°æ®é›†ä¸­çš„æ ·æœ¬æ€»æ•°ã€‚
   - è¿™ä¸ªæ–¹æ³•ç”¨äºç¡®å®šæ•°æ®é›†çš„å¤§å°ï¼Œé€šå¸¸åœ¨åˆ›å»ºæ•°æ®åŠ è½½å™¨ï¼ˆ`DataLoader`ï¼‰æ—¶ä½¿ç”¨ã€‚

### ä½¿ç”¨åœºæ™¯

`Dataset`ç±»é€šå¸¸ç”¨äºä»¥ä¸‹åœºæ™¯ï¼š

- **æ•°æ®åŠ è½½**ï¼šä»æ–‡ä»¶ç³»ç»Ÿã€æ•°æ®åº“æˆ–å…¶ä»–æ¥æºåŠ è½½åŸå§‹æ•°æ®ã€‚
- **æ•°æ®é¢„å¤„ç†**ï¼šå¯¹åŸå§‹æ•°æ®è¿›è¡Œå¿…è¦çš„è½¬æ¢å’Œæ ‡å‡†åŒ–ï¼Œä»¥é€‚åº”æ¨¡å‹è®­ç»ƒçš„éœ€è¦ã€‚
- **æ•°æ®å¢å¼º**ï¼šé€šè¿‡æ—‹è½¬ã€ç¼©æ”¾ã€è£å‰ªç­‰æ–¹æ³•å¢åŠ æ•°æ®çš„å¤šæ ·æ€§ã€‚
- **æ‰¹å¤„ç†**ï¼šå°†å¤šä¸ªæ ·æœ¬ç»„åˆæˆä¸€ä¸ªæ‰¹æ¬¡ï¼Œä»¥ä¾¿äºå¹¶è¡Œå¤„ç†ã€‚

### ç»§æ‰¿å’Œå®ç°

å½“åˆ›å»ºè‡ªå®šä¹‰æ•°æ®é›†æ—¶ï¼Œéœ€è¦ç»§æ‰¿`Dataset`ç±»ï¼Œå¹¶å®ç°ä¸Šè¿°ä¸‰ä¸ªæ–¹æ³•ã€‚ä¾‹å¦‚ï¼š

```python
from torch.utils.data import Dataset

class CustomDataset(Dataset):
    def __init__(self, data_paths, transform=None):
        self.data_paths = data_paths
        self.transform = transform

    def __getitem__(self, index):
        data = load_data(self.data_paths[index])
        if self.transform:
            data = self.transform(data)
        return data

    def __len__(self):
        return len(self.data_paths)
```

### ä¸DataLoaderç»“åˆä½¿ç”¨

`Dataset`å¯¹è±¡é€šå¸¸ä¸`DataLoader`ç±»ç»“åˆä½¿ç”¨ï¼Œ`DataLoader`è´Ÿè´£å°†æ•°æ®é›†å°è£…æˆæ‰¹æ¬¡ï¼Œå¹¶æä¾›å¤šçº¿ç¨‹åŠ è½½ã€æ‰“ä¹±æ•°æ®é¡ºåºç­‰åŠŸèƒ½ã€‚ä¾‹å¦‚ï¼š

```python
from torch.utils.data import DataLoader

dataset = CustomDataset(data_paths, transform=some_transform)
data_loader = DataLoader(dataset, batch_size=32, shuffle=True, num_workers=4)

for batch in data_loader:
    # å¤„ç†æ¯ä¸ªæ‰¹æ¬¡çš„æ•°æ®
    pass
```

é€šè¿‡è¿™ç§æ–¹å¼ï¼Œ`Dataset`ç±»ä¸ºPyTorchç”¨æˆ·æä¾›äº†ä¸€ä¸ªçµæ´»ä¸”å¼ºå¤§çš„æ–¹å¼æ¥å¤„ç†å’ŒåŠ è½½æ•°æ®ï¼Œä½¿å¾—æ•°æ®å‡†å¤‡å·¥ä½œæ›´åŠ é«˜æ•ˆå’Œç³»ç»ŸåŒ–ã€‚



## DataLoader

`DataLoader`æ˜¯PyTorchä¸­çš„ä¸€ä¸ªé‡è¦ç»„ä»¶ï¼Œç”¨äºå°è£…æ•°æ®é›†å¹¶æä¾›æ‰¹é‡åŠ è½½æ•°æ®çš„åŠŸèƒ½ã€‚å®ƒç»§æ‰¿è‡ªPythonçš„è¿­ä»£å™¨ï¼ˆIteratorï¼‰ï¼Œä½¿å¾—æ•°æ®å¯ä»¥ä»¥æ‰¹æ¬¡ï¼ˆbatchï¼‰çš„å½¢å¼é«˜æ•ˆåœ°åŠ è½½å’Œé¢„å¤„ç†ã€‚ä»¥ä¸‹æ˜¯`DataLoader`çš„ä¸€äº›å…³é”®ç‰¹æ€§å’Œä½¿ç”¨æ–¹æ³•ï¼š

### åŸºæœ¬ç‰¹æ€§

1. **æ‰¹é‡åŠ è½½**ï¼š
   - å°†æ•°æ®é›†åˆ†æˆå¤šä¸ªæ‰¹æ¬¡ï¼Œæ¯ä¸ªæ‰¹æ¬¡åŒ…å«å¤šä¸ªæ ·æœ¬ï¼Œè¿™æ ·å¯ä»¥åˆ©ç”¨GPUçš„å¹¶è¡Œè®¡ç®—èƒ½åŠ›ï¼Œæé«˜è®­ç»ƒæ•ˆç‡ã€‚

2. **å¤šçº¿ç¨‹/å¤šè¿›ç¨‹åŠ è½½**ï¼š
   - é€šè¿‡è®¾ç½®`num_workers`å‚æ•°ï¼Œå¯ä»¥å®ç°æ•°æ®çš„å¤šçº¿ç¨‹/å¤šè¿›ç¨‹åŠ è½½ï¼Œä»è€Œå‡å°‘æ•°æ®åŠ è½½æ—¶é—´ã€‚

3. **æ•°æ®æ‰“ä¹±**ï¼š
   - é€šè¿‡è®¾ç½®`shuffle`å‚æ•°ï¼Œå¯ä»¥åœ¨æ¯ä¸ªepochå¼€å§‹æ—¶æ‰“ä¹±æ•°æ®ï¼Œæœ‰åŠ©äºæ¨¡å‹è®­ç»ƒçš„æ³›åŒ–èƒ½åŠ›ã€‚

4. **æ•°æ®é¢„å¤„ç†**ï¼š
   - å¯ä»¥ä¸`Dataset`ç±»ç»“åˆä½¿ç”¨ï¼Œå¯¹æ¯ä¸ªæ ·æœ¬è¿›è¡Œé¢„å¤„ç†ã€‚

5. **è‡ªå®šä¹‰æ‰¹æ¬¡å¤„ç†**ï¼š
   - é€šè¿‡`collate_fn`å‚æ•°ï¼Œå¯ä»¥è‡ªå®šä¹‰å¦‚ä½•å°†å¤šä¸ªæ ·æœ¬åˆå¹¶æˆä¸€ä¸ªæ‰¹æ¬¡ã€‚

### ä½¿ç”¨æ–¹æ³•

ä»¥ä¸‹æ˜¯å¦‚ä½•ä½¿ç”¨`DataLoader`çš„åŸºæœ¬æ­¥éª¤ï¼š

1. **åˆ›å»ºè‡ªå®šä¹‰çš„`Dataset`ç±»**ï¼š
   - å¦‚ä¹‹å‰æ‰€è¿°ï¼Œç»§æ‰¿`Dataset`ç±»å¹¶å®ç°`__getitem__`å’Œ`__len__`æ–¹æ³•ã€‚

2. **å®ä¾‹åŒ–`DataLoader`**ï¼š
   - å°†è‡ªå®šä¹‰çš„`Dataset`å®ä¾‹ä¼ é€’ç»™`DataLoader`ï¼Œå¹¶è®¾ç½®ç›¸å…³å‚æ•°ã€‚

3. **è¿­ä»£`DataLoader`**ï¼š
   - åœ¨è®­ç»ƒå¾ªç¯ä¸­è¿­ä»£`DataLoader`ï¼Œè·å–æ¯ä¸ªæ‰¹æ¬¡çš„æ•°æ®ã€‚

### ç¤ºä¾‹ä»£ç 

```python
from torch.utils.data import DataLoader, Dataset

# å‡è®¾CustomDatasetæ˜¯ä¹‹å‰å®šä¹‰çš„è‡ªå®šä¹‰æ•°æ®é›†ç±»
dataset = CustomDataset(data_paths, transform=some_transform)

# å®ä¾‹åŒ–DataLoader
data_loader = DataLoader(dataset, batch_size=32, shuffle=True, num_workers=4)

# è¿­ä»£DataLoader
for batch in data_loader:
    # å¤„ç†æ¯ä¸ªæ‰¹æ¬¡çš„æ•°æ®
    pass
```

### å‚æ•°è¯¦è§£

- `dataset`ï¼šè¦åŠ è½½çš„æ•°æ®é›†ï¼Œå¿…é¡»æ˜¯`Dataset`å¯¹è±¡ã€‚
- `batch_size`ï¼šæ¯ä¸ªæ‰¹æ¬¡çš„æ ·æœ¬æ•°ã€‚
- `shuffle`ï¼šæ˜¯å¦åœ¨æ¯ä¸ªepochå¼€å§‹æ—¶æ‰“ä¹±æ•°æ®ã€‚
- `num_workers`ï¼šç”¨äºæ•°æ®åŠ è½½çš„å­è¿›ç¨‹æ•°ã€‚è®¾ç½®ä¸ºå¤§äº0çš„å€¼å¯ä»¥åŠ å¿«æ•°æ®åŠ è½½é€Ÿåº¦ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†å¤§è§„æ¨¡æ•°æ®é›†æ—¶ã€‚
- `collate_fn`ï¼šä¸€ä¸ªå‡½æ•°ï¼Œç”¨äºå®šä¹‰å¦‚ä½•å°†å¤šä¸ªæ ·æœ¬åˆå¹¶æˆä¸€ä¸ªæ‰¹æ¬¡ã€‚å¦‚æœè®¾ç½®ä¸º`None`ï¼Œåˆ™ä½¿ç”¨é»˜è®¤çš„åˆå¹¶æ–¹å¼ã€‚

### è‡ªå®šä¹‰`collate_fn`

åœ¨æŸäº›æƒ…å†µä¸‹ï¼Œå¯èƒ½éœ€è¦è‡ªå®šä¹‰å¦‚ä½•å°†å¤šä¸ªæ ·æœ¬åˆå¹¶æˆä¸€ä¸ªæ‰¹æ¬¡ã€‚ä¾‹å¦‚ï¼Œå½“æ•°æ®é›†ä¸­åŒ…å«ä¸åŒå¤§å°çš„å›¾åƒæ—¶ï¼Œå¯èƒ½éœ€è¦åœ¨`collate_fn`ä¸­è¿›è¡Œç‰¹æ®Šçš„å¤„ç†ï¼š

```python
def my_collate_fn(batch):
    images, labels = zip(*batch)
    images = torch.stack(images, dim=0)
    labels = torch.tensor(labels)
    return images, labels

data_loader = DataLoader(dataset, batch_size=32, collate_fn=my_collate_fn)
```

## transform

åœ¨ PyTorch çš„ `torchvision.transforms` æ¨¡å—ä¸­ï¼Œæœ‰ä¸€ä¸ªç°æˆçš„ `Compose` ç±»ï¼Œå®ƒç”¨äºå°†å¤šä¸ªå›¾åƒå˜æ¢æ“ä½œç»„åˆæˆä¸€ä¸ªæ“ä½œåºåˆ—ã€‚è¿™ä¸ªç±»å…è®¸å°†å¤šä¸ªå˜æ¢åŒ…è£…æˆä¸€ä¸ªå•ç‹¬çš„å˜æ¢ã€‚

ä»¥ä¸‹æ˜¯ `torchvision.transforms.Compose` çš„åŸºæœ¬ç”¨æ³•ï¼š

```python
import torchvision.transforms as transforms

# å®šä¹‰ä¸€ç³»åˆ—å˜æ¢æ“ä½œ
transform_list = [
    transforms.Resize(256),
    transforms.CenterCrop(224),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
]

# ä½¿ç”¨Composeå°†å®ƒä»¬ç»„åˆæˆä¸€ä¸ªå˜æ¢
composed_transforms = transforms.Compose(transform_list)

# ç°åœ¨ä½ å¯ä»¥åƒä½¿ç”¨ä¸€ä¸ªæ™®é€šå˜æ¢ä¸€æ ·ä½¿ç”¨å®ƒ
# å‡è®¾imgæ˜¯ä½ çš„PILå›¾åƒ
img_transformed = composed_transforms(img)
```

åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œ`Compose` æ¥å—ä¸€ä¸ªåŒ…å«å¤šä¸ªå˜æ¢çš„åˆ—è¡¨ï¼Œå¹¶ä¸”å½“ä½ è°ƒç”¨ `composed_transforms` å¯¹è±¡æ—¶ï¼Œå®ƒä¼šæŒ‰ç…§åˆ—è¡¨ä¸­çš„é¡ºåºä¾æ¬¡åº”ç”¨è¿™äº›å˜æ¢ã€‚

åœ¨ä½ æä¾›çš„ä»£ç ä¸­ï¼Œ`Compose` ç±»ä¹Ÿæ˜¯ç”¨æ¥å®ç°åŒæ ·çš„åŠŸèƒ½ï¼Œä½†æ˜¯å®ƒæ˜¯è‡ªå®šä¹‰å®ç°çš„ã€‚è‡ªå®šä¹‰çš„ `Compose` ç±»å¯ä»¥åº”ç”¨äºåŒæ—¶éœ€è¦å¯¹å›¾åƒå’Œç›®æ ‡ï¼ˆä¾‹å¦‚æ©ç æˆ–æ ‡ç­¾ï¼‰è¿›è¡Œå˜æ¢çš„åœºæ™¯ã€‚åœ¨ PyTorch çš„å®˜æ–¹å®ç°ä¸­ï¼Œ`Compose` ç±»é€šå¸¸åªå¤„ç†å›¾åƒå˜æ¢ï¼Œè€Œä¸æ¶‰åŠç›®æ ‡å˜æ¢ã€‚è‡ªå®šä¹‰çš„ `Compose` ç±»é€šè¿‡æ¥å—å›¾åƒå’Œç›®æ ‡ä½œä¸ºä¸€å¯¹è¾“å…¥ï¼Œå¹¶ç¡®ä¿æ¯ä¸ªå˜æ¢éƒ½åŒæ—¶åº”ç”¨äºå›¾åƒå’Œç›®æ ‡ï¼Œä»è€Œæ‰©å±•äº†è¿™ä¸€åŠŸèƒ½ã€‚

è‡ªå®šä¹‰ `Compose` ç±»çš„å®ç°å¦‚ä¸‹ï¼š

```python
class Compose(object):
    def __init__(self, transforms):
        self.transforms = transforms

    def __call__(self, image, target):
        for t in self.transforms:
            image, target = t(image, target)
        return image, target
```

åœ¨è¿™ä¸ªè‡ªå®šä¹‰å®ç°ä¸­ï¼Œ`__call__` æ–¹æ³•æ¥å—ä¸€å¯¹å›¾åƒå’Œç›®æ ‡ï¼Œç„¶åé€ä¸ªåº”ç”¨ç»„åˆä¸­çš„æ¯ä¸ªå˜æ¢ï¼Œæœ€åè¿”å›å˜æ¢åçš„å›¾åƒå’Œç›®æ ‡ã€‚è¿™ç§æ–¹å¼ç‰¹åˆ«é€‚ç”¨äºéœ€è¦å¯¹å›¾åƒåŠå…¶å¯¹åº”çš„æ ‡æ³¨æˆ–æ©ç åŒæ—¶è¿›è¡Œç›¸åŒé¢„å¤„ç†çš„åœºæ™¯ã€‚



## call

åœ¨Pythonä¸­ï¼Œ`__call__` æ˜¯ä¸€ä¸ªç‰¹æ®Šçš„æ–¹æ³•ï¼Œä¹Ÿç§°ä¸ºé­”æœ¯æ–¹æ³•æˆ–åŒä¸‹æ–¹æ³•ã€‚å½“å®ƒåœ¨ç±»ä¸­è¢«å®šä¹‰æ—¶ï¼Œå®ƒå…è®¸ä¸€ä¸ªç±»çš„å®ä¾‹è¡¨ç°å¾—åƒä¸€ä¸ªå‡½æ•°ï¼Œå³å¯ä»¥é€šè¿‡ç›´æ¥è°ƒç”¨å®ä¾‹æ¥æ‰§è¡Œ`__call__`æ–¹æ³•ã€‚è¿™é€šå¸¸ç”¨äºå®ç°å‡½æ•°å¼ç¼–ç¨‹çš„æŸäº›ç‰¹æ€§ï¼Œæˆ–è€…ä½¿å¾—å¯¹è±¡å¯ä»¥åƒå‡½æ•°é‚£æ ·è¢«è°ƒç”¨ã€‚

åœ¨PyTorchçš„`transforms`æ¨¡å—ä¸­ï¼Œ`__call__`æ–¹æ³•è¢«å¹¿æ³›ä½¿ç”¨ï¼Œä»¥å®ç°å›¾åƒé¢„å¤„ç†å’Œå¢å¼ºæ“ä½œçš„é“¾å¼è°ƒç”¨ã€‚å…·ä½“æ¥è¯´ï¼Œ`__call__`æ–¹æ³•åœ¨ä»¥ä¸‹åœºæ™¯ä¸­å‘æŒ¥ä½œç”¨ï¼š

1. **ä½¿å¯¹è±¡å¯è°ƒç”¨**ï¼š
   - å½“ä½ æœ‰ä¸€ä¸ªç±»çš„å®ä¾‹ï¼Œå¹¶ä¸”å¸Œæœ›åƒè°ƒç”¨å‡½æ•°é‚£æ ·ä½¿ç”¨å®ƒæ—¶ï¼Œä½ å¯ä»¥åœ¨ç±»ä¸­å®šä¹‰`__call__`æ–¹æ³•ã€‚è¿™æ ·ï¼Œä½ å°±å¯ä»¥ç›´æ¥è°ƒç”¨å®ä¾‹ï¼Œè€Œä¸æ˜¯æ˜¾å¼åœ°è°ƒç”¨å…¶æ–¹æ³•ã€‚

2. **å‚æ•°ä¼ é€’**ï¼š
   - åœ¨`transforms`ä¸­ï¼Œ`__call__`æ–¹æ³•å…è®¸ä½ ä¼ é€’å›¾åƒå’Œç›®æ ‡ï¼ˆå¦‚æ©ç æˆ–æ ‡æ³¨ï¼‰ä½œä¸ºå‚æ•°ï¼Œå¹¶åœ¨å†…éƒ¨æ‰§è¡Œä¸€ç³»åˆ—é¢„å¤„ç†æ“ä½œã€‚

3. **é“¾å¼æ“ä½œ**ï¼š
   - é€šè¿‡åœ¨`__call__`æ–¹æ³•ä¸­å¤„ç†å›¾åƒå’Œç›®æ ‡ï¼Œå¹¶å°†å¤„ç†åçš„å›¾åƒå’Œç›®æ ‡è¿”å›ï¼Œä½ å¯ä»¥å°†å¤šä¸ªé¢„å¤„ç†æ“ä½œé“¾æ¥åœ¨ä¸€èµ·ã€‚è¿™ä½¿å¾—æ•°æ®é¢„å¤„ç†æµç¨‹å˜å¾—ç®€æ´å’Œé«˜æ•ˆã€‚

### ç¤ºä¾‹

ä»¥ä¸‹æ˜¯ä¸€ä¸ªç®€å•çš„ç¤ºä¾‹ï¼Œè¯´æ˜å¦‚ä½•åœ¨è‡ªå®šä¹‰ç±»ä¸­ä½¿ç”¨`__call__`æ–¹æ³•ï¼š

```python
class Greeter:
    def __init__(self, name):
        self.name = name

    def __call__(self):
        print(f"Hello, {self.name}!")

# åˆ›å»ºGreeterå®ä¾‹
greeter = Greeter("Kimi")

# åƒè°ƒç”¨å‡½æ•°ä¸€æ ·è°ƒç”¨å®ä¾‹
greeter()  # è¾“å‡º: Hello, Kimi!
```

åœ¨è¿™ä¸ªç¤ºä¾‹ä¸­ï¼Œ`Greeter`ç±»æœ‰ä¸€ä¸ª`__call__`æ–¹æ³•ï¼Œä½¿å¾—å…¶å®ä¾‹å¯ä»¥åƒå‡½æ•°ä¸€æ ·è¢«è°ƒç”¨ã€‚å½“ä½ è°ƒç”¨`greeter()`æ—¶ï¼Œå®é™…ä¸Šæ˜¯åœ¨è°ƒç”¨`greeter`å®ä¾‹çš„`__call__`æ–¹æ³•ã€‚

### åœ¨PyTorchä¸­çš„ä½¿ç”¨

åœ¨PyTorchçš„`transforms`æ¨¡å—ä¸­ï¼Œ`__call__`æ–¹æ³•çš„ä½¿ç”¨æ–¹å¼å¦‚ä¸‹ï¼š

```python
class RandomHorizontalFlip:
    def __init__(self, flip_prob):
        self.flip_prob = flip_prob

    def __call__(self, image, target):
        if random.random() < self.flip_prob:
            image = F.hflip(image)
            target = F.hflip(target)
        return image, target

# åˆ›å»ºRandomHorizontalFlipå®ä¾‹
flip_transform = RandomHorizontalFlip(0.5)

# è°ƒç”¨å®ä¾‹è¿›è¡Œå›¾åƒç¿»è½¬
image, target = flip_transform(image, target)
```

åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œ`RandomHorizontalFlip`ç±»çš„å®ä¾‹å¯ä»¥åƒå‡½æ•°ä¸€æ ·è¢«è°ƒç”¨ï¼Œæ¥å—å›¾åƒå’Œç›®æ ‡ä½œä¸ºå‚æ•°ï¼Œå¹¶æ ¹æ®å®šä¹‰çš„æ¦‚ç‡è¿›è¡Œæ°´å¹³ç¿»è½¬ã€‚è¿™ç§æ–¹å¼ä½¿å¾—é¢„å¤„ç†æ“ä½œçš„ç»„åˆå˜å¾—éå¸¸çµæ´»å’Œæ–¹ä¾¿ã€‚
