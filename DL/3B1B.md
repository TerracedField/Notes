[toc]

## 词向量

一个词被编码为一个向量：

![P1](pics\P42.png) 

意义接近的词在向量空间上方向也相近：

![P1](pics\P41.png) 

## attention

attention block 通过分析上下文来更新词向量的意思，比如有多义词，在不同的语境下意义不同，词向量标识一个词在某种语境下的含义

在transfomer中词向量互相影响，改变

### 前馈层

![P1](pics\P44.png) 

经过前馈层的处理，前馈层中词向量不再互相影响，而是并行经过前馈层处理

### 处理文本的步骤

#### token

首先把输入的文本转化为token（类比单词，但是也可以是词根或者标点）

![P1](pics\P46.png) 

#### 嵌入矩阵

嵌入矩阵中有原始词汇的词向量，在GPT3中大概是5w个词汇

![P1](pics\P45.png)



意思相近的词如tower和skyscraper（摩天楼）在词向量空间上就比较靠近，同样的男人和女人的词向量在空间上的向量差与国王和皇后的差方向和大小都相近

![P1](pics\P47.png) 

最开始每个输入的文本的词被直接从嵌入矩阵中拉出来

#### 最后

输出的是输入文本下一个token的概率分布

![P1](pics\P48.png) 

#### 解嵌入矩阵

将文本最后一个token对应的向量映射到一个初始词汇库（5w个）对应的向量里面去。

GPT-3词向量的维度是12000左右，只要解嵌入矩阵列数也是这么多，行数为5w就能映射

![P1](pics\P50.png) 

#### softmax

映射后的值通过softmax转化为合理的概率

![P1](pics\P52.png) 

可以通过给softmax的x加分母T来控制，T大的话会使得低值也能得到一些权重，T小那么较大的数值有优势

temperature有点类似模型的想象力那个参数，越小就只联想最可能的那个词，越大词就越发散

#### logits

未经softmax归一化的那个向量

![P1](pics\P53.png)
