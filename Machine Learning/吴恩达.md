[toc]

## w1

### 监督/无监督学习

监督学习：使用标记 数据集 来训练算法，以便对数据进行分类或准确预测结果。

无监督学习：无监督学习是训练机器使用既未分类也未标记的数据的方法。这意味着无法提供训练数据，机器只能自行学习。机器必须能够对数据进行分类，而无需事先提供任何有关数据的信息。

[监督学习or无监督学习？这个问题必须搞清楚 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/137042059)

### 符号

$(x^{(i)},y^{(i)})$：训练集中第i个输入x和第i个输出y

$\hat{y}$：预测出的y

m：训练集样本数量

### 代价函数

$J = \frac{1}{2m}\sum\limits_{i=1}^{m} (\hat{y} - y^{(i)})^2$

平方误差成本，即预测值和真实值差距的平方求个和再除以2m：![P1](pics\P1.png) 

#### 等高线图看代价函数

![P4](pics\P4.png)

等高线中心对应最小的J， 

### 超平面

![P2](pics\P2.png) 

![P3](pics\P3.png) 

[超平面（Hyperplane）浅谈 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/263604941)



就是y，x这些换成x向量了了，如p1中w1，w2对应的A，B，x向量对应x，y

### Loss

每个**分类错误**的样本到超平面的距离之和，自变量为x（投入的样本），参数为w

### 梯度下降

主要看jupyter

虽然晓得了一元线性回归怎么求，但是实际上可以用梯度下降迭代出来。

学习率alpha，w和b的初始值，迭代次数是给定的，具体看吴恩达代码





二维上就是，对函数在x的位置求导，如果导数为正数则后退，负数则前进（都是下降，跟下梯子一样，所以梯度），慢慢蹭过去，从而找到一个函数的最小值（Loss函数）

多维上就是对w求导，即对w向量的每个分量求偏导



#### 学习率 

$\alpha$：学习率（$0<\alpha<1$）
所以不管某处偏导是正的还是负的，都是下降

![P3](pics\P6.png)

随着梯度的下降，偏导数也变小，所以即使alpha固定影响也不大

![P3](pics\P7.png)

#### 实现

$J = \frac{1}{2m}\sum\limits_{i=1}^{m} (f_{w,b}(x^{(i)}) - y^{(i)})^2$
$$
\begin{align*} \text{repeat}&\text{ until convergence:} \; \lbrace \newline
\;  w &= w -  \alpha \frac{\partial J(w,b)}{\partial w} \tag{3}  \; \newline 
 b &= b -  \alpha \frac{\partial J(w,b)}{\partial b}  \newline \rbrace
\end{align*}
$$




$$
\begin{align}
\frac{\partial J(w,b)}{\partial w}  &= \frac{1}{m} \sum\limits_{i = 0}^{m-1} (f_{w,b}(x^{(i)}) - y^{(i)})x^{(i)} \tag{4}\\
  \frac{\partial J(w,b)}{\partial b}  &= \frac{1}{m} \sum\limits_{i = 0}^{m-1} (f_{w,b}(x^{(i)}) - y^{(i)}) \tag{5}\\
\end{align}
$$
因为是对w和b求偏导，所以xi，yi是定值无所谓的

## w2

### 符号

$x_j$：第j个特征

n：特征总数

$x^{(i)}$：第i个输入的特征向量

![P3](pics\P8.png) 

箭头强调x是向量

### 多元线性回归

**Compute Cost With Multiple Variables**

The equation for the cost function with multiple variables $J(\mathbf{w},b)$ is:
$$J(\mathbf{w},b) = \frac{1}{2m} \sum\limits_{i = 0}^{m-1} (f_{\mathbf{w},b}(\mathbf{x}^{(i)}) - y^{(i)})^2 \tag{3}$$ 
where:
$$ f_{\mathbf{w},b}(\mathbf{x}^{(i)}) = \mathbf{w} \cdot \mathbf{x}^{(i)} + b  \tag{4} $$ 


In contrast to previous labs, $\mathbf{w}$ and $\mathbf{x}^{(i)}$ are vectors rather than scalars supporting multiple features.



**Gradient Descent With Multiple Variables**

Gradient descent for multiple variables:

$$\begin{align*} \text{repeat}&\text{ until convergence:} \; \lbrace \newline\;
& w_j = w_j -  \alpha \frac{\partial J(\mathbf{w},b)}{\partial w_j} \tag{5}  \; & \text{for j = 0..n-1}\newline
&b\ \ = b -  \alpha \frac{\partial J(\mathbf{w},b)}{\partial b}  \newline \rbrace
\end{align*}$$

where, n is the number of features, parameters $w_j$,  $b$, are updated simultaneously and where  

$$
\frac{\partial J(\mathbf{w},b)}{\partial w_j}  &= \frac{1}{m} \sum\limits_{i = 0}^{m-1} (f_{\mathbf{w},b}(\mathbf{x}^{(i)}) - y^{(i)})x_{j}^{(i)} \tag{6}  \\
\frac{\partial J(\mathbf{w},b)}{\partial b}  &= \frac{1}{m} \sum\limits_{i = 0}^{m-1} (f_{\mathbf{w},b}(\mathbf{x}^{(i)}) - y^{(i)}) \tag{7}
$$
* m is the number of training examples in the data set

*  $f_{\mathbf{w},b}(\mathbf{x}^{(i)})$ is the model's prediction, while $y^{(i)}$ is the target value



就是对每个w向量分量求偏导，然后梯度下降

### 特征缩放

作用：使梯度下降更快

对于根据卧室数量和房屋平方来预测房价的线性模型，如果没有标准化特征，那么卧室数量的w0很大，房屋面积的w1很小，所以w1微小的变化对成本函数J的影响就很大，反正就是不好（因为w1小则导数大，实际影响感觉不会很大。。。）

但是查到的：多数的[分类器](https://baike.baidu.com/item/分类器/0?fromModule=lemma_inlink)利用两点间的距离计算两点的差异，若其中一 个特征具有非常广的范围，那两点间的差异就会被该特征左右



![P3](pics\P9.png) 

就是标准化x，使得每个x分量都在0-1之间

#### 均值归一化

![P3](pics\P10.png) 

#### Z-score

![P3](pics\P11.png)

$\sigma$：标准差

#### 不同的缩放情况

![P3](pics\P12.png)

### 判断梯度下降是否收敛

1. 看图，代价函数J随w，b迭代次数不动时就收敛
2. 设置一个epsilon，是某个小的值如0.001，J随迭代次数变化的值小于epsilon时就判断为收敛（很难找到合适的epsilon，不推荐）

## w3

### sigmoid function

将原来f的值映射到0 - 1之间，还是通过改变w和b的大小来拟合

Logistic Regression

<img align="left" src="pics/C1_W3_LogisticRegression_right.png"     style=" width:300px; padding: 10px; " > A logistic regression model applies the sigmoid to the familiar linear regression model as shown below:

$$ f_{\mathbf{w},b}(\mathbf{x}^{(i)}) = g(\mathbf{w} \cdot \mathbf{x}^{(i)} + b ) \tag{2} $$ 

  where

  $g(z) = \frac{1}{1+e^{-z}}\tag{3}$







### loss function

## Logistic Loss Function


![P3](pics\C1_W3_LogisticLoss_a.png) 

![P3](pics\C1_W3_LogisticLoss_b.png) 

![P3](pics\C1_W3_LogisticLoss_c.png) 

Logistic Regression uses a loss function more suited to the task of categorization where the target is 0 or 1 rather than any number. 

>**Definition Note:**   In this course, these definitions are used:  
>**Loss** is a measure of the difference of a single example to its target value while the  
>**Cost** is a measure of the losses over the training set


This is defined: 
* $loss(f_{\mathbf{w},b}(\mathbf{x}^{(i)}), y^{(i)})$ is the cost for a single data point, which is:

$$

$$



*  $f_{\mathbf{w},b}(\mathbf{x}^{(i)})$ is the model's prediction, while $y^{(i)}$ is the target value.

*  $f_{\mathbf{w},b}(\mathbf{x}^{(i)}) = g(\mathbf{w} \cdot\mathbf{x}^{(i)}+b)$ where function $g$ is the sigmoid function.

The defining feature of this loss function is the fact that it uses two separate curves. One for the case when the target is zero or ($y=0$) and another for when the target is one ($y=1$). Combined, these curves provide the behavior useful for a loss function, namely, being zero when the prediction matches the target and rapidly increasing in value as the prediction differs from the target. Consider the curves below:

可以简化为：

$$loss(f_{\mathbf{w},b}(\mathbf{x}^{(i)}), y^{(i)}) = (-y^{(i)} \log\left(f_{\mathbf{w},b}\left( \mathbf{x}^{(i)} \right) \right) - \left( 1 - y^{(i)}\right) \log \left( 1 - f_{\mathbf{w},b}\left( \mathbf{x}^{(i)} \right) \right)$$
